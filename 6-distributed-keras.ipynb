{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our code, locally to begin with, using CMLE.\n",
    "\n",
    "Here are the changes we need to make convert our code to a python script file in folder called trainer and convert that folder into a package by adding an __init__.py script file.\n",
    "\n",
    "After that we'll test the code from the command line.\n",
    "\n",
    "Then we'll run the full, distributed job on CMLE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some housekeeping, we need to delete the output folder we use locally between runs (in case you run this notebook multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf trainer && rm -rf abalone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"DNNRegressor with custom estimator for abalone dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "  'length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n",
    "  'viscera_weight', 'shell_weight', 'num_rings'\n",
    "]\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "PREDICTED_COLUMN = 'num_rings'\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('length'),\n",
    "    tf.feature_column.numeric_column('diameter'),\n",
    "    tf.feature_column.numeric_column('height'),\n",
    "    tf.feature_column.numeric_column('whole_weight'),\n",
    "    tf.feature_column.numeric_column('shucked_weight'),\n",
    "    tf.feature_column.numeric_column('viscera_weight'),\n",
    "    tf.feature_column.numeric_column('shell_weight'),\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - {PREDICTED_COLUMN}\n",
    "\n",
    "def parse_csv(rows_string_tensor):\n",
    "  columns = tf.decode_csv(rows_string_tensor, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "  features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "  for col in UNUSED_COLUMNS:\n",
    "    features.pop(col)\n",
    "\n",
    "  for key, value in six.iteritems(features):\n",
    "    features[key] = tf.expand_dims(features[key], -1)\n",
    "  return features\n",
    "\n",
    "def generate_input_fn(filenames,\n",
    "                      num_epochs=None,\n",
    "                      shuffle=True,\n",
    "                      skip_header_lines=0,\n",
    "                      batch_size=64):\n",
    "  \n",
    "  def _input_fn():\n",
    "  \n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=shuffle)\n",
    "    reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
    "\n",
    "    _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "    features = parse_csv(rows)\n",
    "\n",
    "    if shuffle:\n",
    "      features = tf.train.shuffle_batch(\n",
    "          features,\n",
    "          batch_size,\n",
    "          min_after_dequeue=2 * batch_size + 1,\n",
    "          capacity=batch_size * 10,\n",
    "          num_threads=multiprocessing.cpu_count(),\n",
    "          enqueue_many=True,\n",
    "          allow_smaller_final_batch=True\n",
    "      )\n",
    "    else:\n",
    "      features = tf.train.batch(\n",
    "          features,\n",
    "          batch_size,\n",
    "          capacity=batch_size * 10,\n",
    "          num_threads=multiprocessing.cpu_count(),\n",
    "          enqueue_many=True,\n",
    "          allow_smaller_final_batch=True\n",
    "      )\n",
    "\n",
    "    return features, features.pop(PREDICTED_COLUMN)\n",
    "  \n",
    "  return _input_fn\n",
    "\n",
    "def generate_model_fn(learning_rate):\n",
    "    \n",
    "    def _model_fn(mode, features, labels):\n",
    "\n",
    "        (length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight) = INPUT_COLUMNS\n",
    "\n",
    "        transformed_columns = [\n",
    "            length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight\n",
    "        ]\n",
    "\n",
    "        inputs = tf.feature_column.input_layer(features, transformed_columns)\n",
    "\n",
    "        first_hidden_layer = Dense(10, activation='relu')(inputs)\n",
    "        second_hidden_layer = Dense(10, activation='relu')(first_hidden_layer)\n",
    "        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n",
    "\n",
    "        if mode in (Modes.PREDICT, Modes.EVAL):\n",
    "          predictions = tf.reshape(output_layer, [-1])\n",
    "          predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "        if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "          loss = tf.losses.mean_squared_error(labels, output_layer)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "          train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "        \n",
    "        if mode == Modes.TRAIN:\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        if mode == Modes.EVAL:\n",
    "          eval_metric_ops = {\n",
    "              \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "                  tf.cast(labels, tf.float32), predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "        if mode == Modes.PREDICT:\n",
    "          export_outputs = {\n",
    "              'prediction': tf.estimator.export.RegressionOutput(predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions_dict, export_outputs=export_outputs)\n",
    "    \n",
    "    return _model_fn\n",
    "  \n",
    "def generate_experiment_fn(**experiment_args):  \n",
    "  \n",
    "  def _experiment_fn(run_config, hparams):\n",
    "\n",
    "    train_input = generate_input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size,\n",
    "    )\n",
    "\n",
    "    test_input = generate_input_fn(\n",
    "      hparams.eval_files,\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        tf.estimator.Estimator(\n",
    "            generate_model_fn(learning_rate=hparams.learning_rate),\n",
    "            config=run_config\n",
    "        ),\n",
    "        train_input_fn=train_input,\n",
    "        eval_input_fn=test_input,\n",
    "        **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn\n",
    "\n",
    "def example_serving_input_fn():\n",
    "  \"\"\"Build the serving inputs.\"\"\"\n",
    "  example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "  )\n",
    "  features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n",
    "  )\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS or local paths to training data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate for the optimizer',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    # Experiment arguments\n",
    "    parser.add_argument(\n",
    "        '--eval-delay-secs',\n",
    "        help='How long to wait before running first evaluation',\n",
    "        default=10,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min-eval-frequency',\n",
    "        help='Minimum number of training steps between evaluations',\n",
    "        default=1,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help=\"\"\"\\\n",
    "        Number of steps to run evalution for at each checkpoint.\n",
    "        If unspecified will run until the input from --eval-files is exhausted\n",
    "        \"\"\",\n",
    "        default=None,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    '''args = parser.parse_args([\n",
    "        '--train-files', 'gs://smiling-beaming-abalone/abalone_train.csv',\n",
    "        '--eval-files', 'gs://smiling-beaming-abalone/abalone_test.csv',\n",
    "        '--job-dir', 'abalone_output',\n",
    "        '--train-steps', '5000',\n",
    "        '--eval-steps', '100'\n",
    "      ])'''\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Run the training job\n",
    "    # learn_runner pulls configuration information from environment\n",
    "    # variables using tf.learn.RunConfig and uses this configuration\n",
    "    # to conditionally execute Experiment, or param server code\n",
    "    learn_runner.run(\n",
    "        generate_experiment_fn(\n",
    "            min_eval_frequency=args.min_eval_frequency,\n",
    "            eval_delay_secs=args.eval_delay_secs,\n",
    "            train_steps=args.train_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "                example_serving_input_fn,\n",
    "                exports_to_keep=1\n",
    "            )]\n",
    "        ),\n",
    "        run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n",
    "        hparams=hparam.HParams(**args.__dict__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'abalone_output', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5d62aba290>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_session_config': None}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-08-03 18:12:31.062091: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-03 18:12:31.062180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-03 18:12:31.062193: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-03 18:12:31.062200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-03 18:12:31.062206: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:loss = 33.3397, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-18:12:31\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-18:12:33\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 106.731, rmse = 10.3311\n",
      "INFO:tensorflow:Validation (step 1): loss = 106.731, global_step = 1, rmse = 10.3311\n",
      "INFO:tensorflow:global_step/sec: 41.65\n",
      "INFO:tensorflow:loss = 1.849, step = 101 (2.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.349\n",
      "INFO:tensorflow:loss = 5.67544, step = 201 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.785\n",
      "INFO:tensorflow:loss = 1.11812, step = 301 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 505.137\n",
      "INFO:tensorflow:loss = 0.102634, step = 401 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 576.812\n",
      "INFO:tensorflow:loss = 24.8444, step = 501 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.452\n",
      "INFO:tensorflow:loss = 3.95816, step = 601 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.374\n",
      "INFO:tensorflow:loss = 0.889916, step = 701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.932\n",
      "INFO:tensorflow:loss = 1.12361, step = 801 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.184\n",
      "INFO:tensorflow:loss = 21.2184, step = 901 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.673\n",
      "INFO:tensorflow:loss = 9.47407, step = 1001 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 580.473\n",
      "INFO:tensorflow:loss = 2.99833, step = 1101 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.482\n",
      "INFO:tensorflow:loss = 0.939891, step = 1201 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.99\n",
      "INFO:tensorflow:loss = 1.58475, step = 1301 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 583.546\n",
      "INFO:tensorflow:loss = 0.0848745, step = 1401 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 580.565\n",
      "INFO:tensorflow:loss = 5.25113, step = 1501 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.396\n",
      "INFO:tensorflow:loss = 0.219022, step = 1601 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.02\n",
      "INFO:tensorflow:loss = 1.84931, step = 1701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 588.606\n",
      "INFO:tensorflow:loss = 1.9199, step = 1801 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.499\n",
      "INFO:tensorflow:loss = 0.232887, step = 1901 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 593.127\n",
      "INFO:tensorflow:loss = 3.62392, step = 2001 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.344\n",
      "INFO:tensorflow:loss = 0.089225, step = 2101 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.466\n",
      "INFO:tensorflow:loss = 11.323, step = 2201 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.331\n",
      "INFO:tensorflow:loss = 2.17388, step = 2301 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.663\n",
      "INFO:tensorflow:loss = 3.34486, step = 2401 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.669\n",
      "INFO:tensorflow:loss = 1.07935, step = 2501 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.453\n",
      "INFO:tensorflow:loss = 1.59146, step = 2601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 590.549\n",
      "INFO:tensorflow:loss = 23.1178, step = 2701 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.396\n",
      "INFO:tensorflow:loss = 8.86482, step = 2801 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.302\n",
      "INFO:tensorflow:loss = 12.5108, step = 2901 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.129\n",
      "INFO:tensorflow:loss = 2.61142, step = 3001 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.713\n",
      "INFO:tensorflow:loss = 4.35623, step = 3101 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.209\n",
      "INFO:tensorflow:loss = 0.0349848, step = 3201 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.209\n",
      "INFO:tensorflow:loss = 6.022, step = 3301 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.84\n",
      "INFO:tensorflow:loss = 0.241793, step = 3401 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 529.93\n",
      "INFO:tensorflow:loss = 0.452591, step = 3501 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.29\n",
      "INFO:tensorflow:loss = 0.16297, step = 3601 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.788\n",
      "INFO:tensorflow:loss = 0.0912593, step = 3701 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.114\n",
      "INFO:tensorflow:loss = 28.9097, step = 3801 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.762\n",
      "INFO:tensorflow:loss = 1.01807, step = 3901 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.93\n",
      "INFO:tensorflow:loss = 8.18962, step = 4001 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.626\n",
      "INFO:tensorflow:loss = 16.6479, step = 4101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.57\n",
      "INFO:tensorflow:loss = 15.9011, step = 4201 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.407\n",
      "INFO:tensorflow:loss = 0.844676, step = 4301 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.762\n",
      "INFO:tensorflow:loss = 0.0322818, step = 4401 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.197\n",
      "INFO:tensorflow:loss = 1.28816, step = 4501 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.176\n",
      "INFO:tensorflow:loss = 5.89572, step = 4601 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.393\n",
      "INFO:tensorflow:loss = 0.144549, step = 4701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.763\n",
      "INFO:tensorflow:loss = 0.109821, step = 4801 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.764\n",
      "INFO:tensorflow:loss = 19.1559, step = 4901 (0.182 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 14.1737.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-18:12:42\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-18:12:44\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 6.17021, rmse = 2.48399\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: abalone_output/export/Servo/1501783964/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local train --package-path trainer \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code on CMLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [abalone_170803_181245] submitted successfully.\n",
      "INFO\t2017-08-03 18:12:47 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2017-08-03 18:12:47 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2017-08-03 18:12:47 +0000\tservice\t\tJob abalone_170803_181245 is queued.\n",
      "INFO\t2017-08-03 18:13:00 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2017-08-03 18:13:01 +0000\tservice\t\tWaiting for TensorFlow to start.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"ps\", \"index\": 0} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"ps\", \"index\": 1} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"worker\", \"index\": 0} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"ps\", \"index\": 2} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"master\", \"index\": 0} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"worker\", \"index\": 2} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"worker\", \"index\": 3} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-1d39c7b42c-0:2222\"], \"ps\": [\"ps-1d39c7b42c-0:2222\", \"ps-1d39c7b42c-1:2222\", \"ps-1d39c7b42c-2:2222\"], \"worker\": [\"worker-1d39c7b42c-0:2222\", \"worker-1d39c7b42c-1:2222\", \"worker-1d39c7b42c-2:2222\", \"worker-1d39c7b42c-3:2222\"]} --task={\"type\": \"worker\", \"index\": 1} --job={\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170803_181245\"\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tps-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tworker-replica-3\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:02 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-3\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-3\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tmaster-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tmaster-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tps-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170803_181245/packages/c7bb3679bdc32d1d859a2e38c5dccecc63ab69540a5a6d801f4ff46ad5b0447a/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:03 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tcreating '/tmp/tmpqGC43qpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tcreating '/tmp/tmpBcR68fpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tcreating '/tmp/tmpZMhupBpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tcreating '/tmp/tmpNPtri1pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-3\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tcreating '/tmp/tmprr35CWpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tcreating '/tmp/tmp57IFmvpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tcreating '/tmp/tmpnZEpbupip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tcreating '/tmp/tmpBEAsPxpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tps-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:04 +0000\tworker-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tmaster-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tcreating '/tmp/tmpJugODBpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-3\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tcreating '/tmp/tmpI7YTx7pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tcreating '/tmp/tmpnJk0gipip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tmaster-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tcreating '/tmp/tmpPiA_N3pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:05 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:05 +0000\tworker-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tcreating '/tmp/tmpSRHu_vpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpyppLMypip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tcreating '/tmp/tmpViqXH1pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:06 +0000\tmaster-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tcreating '/tmp/tmpYfVuH3pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-03 18:13:06 +0000\tworker-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-03 18:13:06 +0000\tps-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170803_181245\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff07353ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t, '_num_worker_replicas': 4, '_task_id': 3, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-1d39c7b42c-3:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.163032: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.163068: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.172488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.172538: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.172546: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> localhost:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\t2017-08-03 18:13:07.173187: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-3\t\tWaiting 14 secs before starting training.\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb8817fae10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-1d39c7b42c-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.452871: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.452906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.461892: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.461926: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.461936: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> localhost:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\t2017-08-03 18:13:07.462402: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-2\t\tWaiting 9 secs before starting training.\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f71415bee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-1d39c7b42c-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.638749: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.638804: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.648658: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.648708: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.648717: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tworker-replica-0\t\t2017-08-03 18:13:07.649329: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3abd13ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-1d39c7b42c-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.663885: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.663922: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.674985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.675039: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> localhost:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.675048: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-1\t\t2017-08-03 18:13:07.675895: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f594a57ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-1d39c7b42c-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:07 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.808727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.809358: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.822996: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.823065: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> localhost:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.823074: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:07 +0000\tps-replica-2\t\t2017-08-03 18:13:07.823669: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f08387bee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-1d39c7b42c-1:2222', '_session_config': None}\n",
      "INFO\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd562ffae10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://master-1d39c7b42c-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.059754: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.059825: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.072503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.072571: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\tWaiting 4 secs before starting training.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.072580: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> localhost:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tworker-replica-1\t\t2017-08-03 18:13:08.073137: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tMonitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.109353: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.109406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO\t2017-08-03 18:13:08 +0000\tworker-replica-0\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.121887: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.121950: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-1d39c7b42c-0:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.121959: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\t2017-08-03 18:13:08.122473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-03 18:13:08 +0000\tps-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170803_181245', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe8a797ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-1d39c7b42c-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-03 18:13:08 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.394075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.394130: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.406356: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-1d39c7b42c-0:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.406416: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222, 1 -> ps-1d39c7b42c-1:2222, 2 -> ps-1d39c7b42c-2:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.406427: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-1d39c7b42c-0:2222, 1 -> worker-1d39c7b42c-1:2222, 2 -> worker-1d39c7b42c-2:2222, 3 -> worker-1d39c7b42c-3:2222}\n",
      "ERROR\t2017-08-03 18:13:08 +0000\tps-replica-0\t\t2017-08-03 18:13:08.407202: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-03 18:13:08 +0000\tmaster-replica-0\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\t2017-08-03 18:13:09.214838: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 0705f3978957da4e with config: \n",
      "ERROR\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\tgpu_options {\n",
      "ERROR\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\t}\n",
      "ERROR\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-03 18:13:09 +0000\tworker-replica-0\t\tWaiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, OptimizeLoss/learning_rate, ready: None\n",
      "ERROR\t2017-08-03 18:13:10 +0000\tmaster-replica-0\t\t2017-08-03 18:13:10.415744: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session e768800f0c1f8a6d with config: \n",
      "ERROR\t2017-08-03 18:13:10 +0000\tmaster-replica-0\t\tgpu_options {\n",
      "ERROR\t2017-08-03 18:13:10 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-03 18:13:10 +0000\tmaster-replica-0\t\t}\n",
      "ERROR\t2017-08-03 18:13:10 +0000\tmaster-replica-0\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-03 18:13:12 +0000\tmaster-replica-0\t\tSaving checkpoints for 0 into gs://smiling-beaming-abalone/abalone_170803_181245/model.ckpt.\n",
      "INFO\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\t2017-08-03 18:13:13.483604: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session c232316fffc1c87c with config: \n",
      "ERROR\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\tgpu_options {\n",
      "ERROR\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\t}\n",
      "ERROR\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-03 18:13:13 +0000\tworker-replica-1\t\tloss = 96.8894, step = 1\n",
      "INFO\t2017-08-03 18:13:14 +0000\tworker-replica-1\t\tloss = 0.038779, step = 101 (0.407 sec)\n",
      "INFO\t2017-08-03 18:13:14 +0000\tworker-replica-1\t\tloss = 4.97285, step = 201 (0.418 sec)\n",
      "INFO\t2017-08-03 18:13:14 +0000\tworker-replica-1\t\tloss = 0.175386, step = 301 (0.375 sec)\n",
      "INFO\t2017-08-03 18:13:15 +0000\tworker-replica-1\t\tloss = 7.05462, step = 401 (0.410 sec)\n",
      "INFO\t2017-08-03 18:13:15 +0000\tworker-replica-1\t\tloss = 1.82421, step = 501 (0.418 sec)\n",
      "INFO\t2017-08-03 18:13:16 +0000\tworker-replica-1\t\tloss = 2.97047, step = 601 (0.408 sec)\n",
      "INFO\t2017-08-03 18:13:16 +0000\tworker-replica-1\t\tloss = 3.0068, step = 701 (0.426 sec)\n",
      "INFO\t2017-08-03 18:13:16 +0000\tworker-replica-1\t\tloss = 0.0373978, step = 801 (0.416 sec)\n",
      "INFO\t2017-08-03 18:13:17 +0000\tworker-replica-1\t\tloss = 2.43428, step = 901 (0.429 sec)\n",
      "INFO\t2017-08-03 18:13:17 +0000\tworker-replica-1\t\tloss = 1.98663, step = 1001 (0.426 sec)\n",
      "INFO\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\t2017-08-03 18:13:17.927645: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session fae8e9395fa8e5cd with config: \n",
      "ERROR\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\tgpu_options {\n",
      "ERROR\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\t}\n",
      "ERROR\t2017-08-03 18:13:17 +0000\tworker-replica-2\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-2\t\tloss = 7.94241, step = 1051\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-1\t\tloss = 5.74369, step = 1127 (0.393 sec)\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-2\t\tloss = 0.418709, step = 1251 (0.306 sec)\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-1\t\tloss = 0.419927, step = 1372 (0.414 sec)\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-2\t\tloss = 1.04466, step = 1420 (0.274 sec)\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-2\t\tloss = 8.4466, step = 1586 (0.272 sec)\n",
      "INFO\t2017-08-03 18:13:18 +0000\tworker-replica-1\t\tloss = 0.680807, step = 1622 (0.415 sec)\n",
      "INFO\t2017-08-03 18:13:19 +0000\tworker-replica-2\t\tloss = 2.07614, step = 1757 (0.295 sec)\n",
      "INFO\t2017-08-03 18:13:19 +0000\tworker-replica-1\t\tloss = 40.2557, step = 1865 (0.420 sec)\n",
      "INFO\t2017-08-03 18:13:19 +0000\tworker-replica-2\t\tloss = 19.4477, step = 1926 (0.286 sec)\n",
      "INFO\t2017-08-03 18:13:19 +0000\tworker-replica-2\t\tloss = 6.60219, step = 2090 (0.262 sec)\n",
      "INFO\t2017-08-03 18:13:19 +0000\tworker-replica-1\t\tloss = 2.40801, step = 2114 (0.402 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-2\t\tloss = 13.643, step = 2259 (0.278 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-1\t\tloss = 0.121171, step = 2356 (0.409 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-2\t\tloss = 6.37788, step = 2433 (0.302 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-1\t\tloss = 2.1686, step = 2589 (0.392 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-2\t\tloss = 18.3992, step = 2609 (0.296 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-2\t\tloss = 7.06754, step = 2782 (0.270 sec)\n",
      "INFO\t2017-08-03 18:13:20 +0000\tworker-replica-1\t\tloss = 0.180844, step = 2825 (0.375 sec)\n",
      "INFO\t2017-08-03 18:13:21 +0000\tworker-replica-2\t\tloss = 4.46558, step = 2957 (0.295 sec)\n",
      "INFO\t2017-08-03 18:13:21 +0000\tworker-replica-1\t\tloss = 2.11033, step = 3059 (0.392 sec)\n",
      "INFO\t2017-08-03 18:13:21 +0000\tworker-replica-2\t\tloss = 5.4298, step = 3125 (0.274 sec)\n",
      "INFO\t2017-08-03 18:13:21 +0000\tworker-replica-2\t\tloss = 7.10516, step = 3294 (0.275 sec)\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.2 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path trainer \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
