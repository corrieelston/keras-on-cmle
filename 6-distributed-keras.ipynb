{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to do a little Datalab-fu to get things running on CMLE.  The essence is that once you get to the point of a working, production-grade model you'll flip from Datalab to coding in Python script files because CMLE requires this (you need to supply a python package and module to the gcloud CLI.  Personally, what I do is once I have the code in the previous notebook running in Datalab I download the script as a Python script file and switch to my editor of choice (usually vi or emacs because I don't have much left to do in essence).\n",
    "\n",
    "However, what I'll do here is create the required files via Datalab:\n",
    "\n",
    "1. Some cleanup - in case you are running this notebook multiple times - by deleting the local directories for the Python package and the model training output.\n",
    "2. Create a directory for our Python package (called trainer).\n",
    "3. Create an (empty) __init__.py file in the trainer directory (which can be empty but serves to designate the directory as a package).\n",
    "4. Take the exact same code as the last notebook (with the exception that we'll read all the arguments from the command line - in the last notebook we hardcoded some defaults) and save it in a file called task in the trainer directory.\n",
    "\n",
    "Then we can run train and evaluate the mode locally and on CMLE proper.\n",
    "\n",
    "To run locally we execute the following.\n",
    "\n",
    "```\n",
    "gcloud ml-engine local train --package-path trainer \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100\n",
    "```\n",
    "\n",
    "Which is self explanatory.  To run on CMLE we execute the following.\n",
    "\n",
    "```\n",
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.2 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path trainer \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100\n",
    "```\n",
    "\n",
    "The only difference in this case is that we submit the job to the CMLE managed service and we choose a scale tier to indicate the amount of compute horsepower we want to throw at the problem.  In this case, we're using STANDARD_1 which gives us a master, three workers and three parameter servers (all CPU-based) which is teh smallest, standard footprint that gives us distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some housekeeping, we need to delete the output folder we use locally between runs (in case you run this notebook multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf trainer && rm -rf abalone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"DNNRegressor with custom estimator for abalone dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "  'length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n",
    "  'viscera_weight', 'shell_weight', 'num_rings'\n",
    "]\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "PREDICTED_COLUMN = 'num_rings'\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('length'),\n",
    "    tf.feature_column.numeric_column('diameter'),\n",
    "    tf.feature_column.numeric_column('height'),\n",
    "    tf.feature_column.numeric_column('whole_weight'),\n",
    "    tf.feature_column.numeric_column('shucked_weight'),\n",
    "    tf.feature_column.numeric_column('viscera_weight'),\n",
    "    tf.feature_column.numeric_column('shell_weight'),\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - {PREDICTED_COLUMN}\n",
    "\n",
    "def parse_csv(rows_string_tensor):\n",
    "  columns = tf.decode_csv(rows_string_tensor, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "  features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "  for col in UNUSED_COLUMNS:\n",
    "    features.pop(col)\n",
    "\n",
    "  for key, value in six.iteritems(features):\n",
    "    features[key] = tf.expand_dims(features[key], -1)\n",
    "  return features\n",
    "\n",
    "def generate_input_fn(filenames,\n",
    "                      num_epochs=None,\n",
    "                      shuffle=True,\n",
    "                      skip_header_lines=0,\n",
    "                      batch_size=64):\n",
    "  \n",
    "  def _input_fn():\n",
    "  \n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=shuffle)\n",
    "    reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
    "\n",
    "    _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "    features = parse_csv(rows)\n",
    "\n",
    "    if shuffle:\n",
    "      features = tf.train.shuffle_batch(\n",
    "          features,\n",
    "          batch_size,\n",
    "          min_after_dequeue=2 * batch_size + 1,\n",
    "          capacity=batch_size * 10,\n",
    "          num_threads=multiprocessing.cpu_count(),\n",
    "          enqueue_many=True,\n",
    "          allow_smaller_final_batch=True\n",
    "      )\n",
    "    else:\n",
    "      features = tf.train.batch(\n",
    "          features,\n",
    "          batch_size,\n",
    "          capacity=batch_size * 10,\n",
    "          num_threads=multiprocessing.cpu_count(),\n",
    "          enqueue_many=True,\n",
    "          allow_smaller_final_batch=True\n",
    "      )\n",
    "\n",
    "    return features, features.pop(PREDICTED_COLUMN)\n",
    "  \n",
    "  return _input_fn\n",
    "\n",
    "def generate_model_fn(learning_rate):\n",
    "    \n",
    "    def _model_fn(mode, features, labels):\n",
    "\n",
    "        (length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight) = INPUT_COLUMNS\n",
    "\n",
    "        transformed_columns = [\n",
    "            length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight\n",
    "        ]\n",
    "\n",
    "        inputs = tf.feature_column.input_layer(features, transformed_columns)\n",
    "\n",
    "        first_hidden_layer = Dense(10, activation='relu')(inputs)\n",
    "        second_hidden_layer = Dense(10, activation='relu')(first_hidden_layer)\n",
    "        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n",
    "\n",
    "        if mode in (Modes.PREDICT, Modes.EVAL):\n",
    "          predictions = tf.reshape(output_layer, [-1])\n",
    "          predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "        if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "          loss = tf.losses.mean_squared_error(labels, output_layer)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "          train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "        \n",
    "        if mode == Modes.TRAIN:\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        if mode == Modes.EVAL:\n",
    "          eval_metric_ops = {\n",
    "              \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "                  tf.cast(labels, tf.float32), predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "        if mode == Modes.PREDICT:\n",
    "          export_outputs = {\n",
    "              'prediction': tf.estimator.export.RegressionOutput(predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions_dict, export_outputs=export_outputs)\n",
    "    \n",
    "    return _model_fn\n",
    "  \n",
    "def generate_experiment_fn(**experiment_args):  \n",
    "  \n",
    "  def _experiment_fn(run_config, hparams):\n",
    "\n",
    "    train_input = generate_input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size,\n",
    "    )\n",
    "\n",
    "    test_input = generate_input_fn(\n",
    "      hparams.eval_files,\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        tf.estimator.Estimator(\n",
    "            generate_model_fn(learning_rate=hparams.learning_rate),\n",
    "            config=run_config\n",
    "        ),\n",
    "        train_input_fn=train_input,\n",
    "        eval_input_fn=test_input,\n",
    "        **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn\n",
    "\n",
    "def example_serving_input_fn():\n",
    "  \"\"\"Build the serving inputs.\"\"\"\n",
    "  example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "  )\n",
    "  features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n",
    "  )\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS or local paths to training data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate for the optimizer',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    # Experiment arguments\n",
    "    parser.add_argument(\n",
    "        '--eval-delay-secs',\n",
    "        help='How long to wait before running first evaluation',\n",
    "        default=10,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min-eval-frequency',\n",
    "        help='Minimum number of training steps between evaluations',\n",
    "        default=1,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help=\"\"\"\\\n",
    "        Number of steps to run evalution for at each checkpoint.\n",
    "        If unspecified will run until the input from --eval-files is exhausted\n",
    "        \"\"\",\n",
    "        default=None,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    '''args = parser.parse_args([\n",
    "        '--train-files', 'gs://smiling-beaming-abalone/abalone_train.csv',\n",
    "        '--eval-files', 'gs://smiling-beaming-abalone/abalone_test.csv',\n",
    "        '--job-dir', 'abalone_output',\n",
    "        '--train-steps', '5000',\n",
    "        '--eval-steps', '100'\n",
    "      ])'''\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Run the training job\n",
    "    # learn_runner pulls configuration information from environment\n",
    "    # variables using tf.learn.RunConfig and uses this configuration\n",
    "    # to conditionally execute Experiment, or param server code\n",
    "    learn_runner.run(\n",
    "        generate_experiment_fn(\n",
    "            min_eval_frequency=args.min_eval_frequency,\n",
    "            eval_delay_secs=args.eval_delay_secs,\n",
    "            train_steps=args.train_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "                example_serving_input_fn,\n",
    "                exports_to_keep=1\n",
    "            )]\n",
    "        ),\n",
    "        run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n",
    "        hparams=hparam.HParams(**args.__dict__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'abalone_output', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5fff97a290>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_session_config': None}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-08-07 20:56:31.162338: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-07 20:56:31.162425: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-07 20:56:31.162438: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-07 20:56:31.162446: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-07 20:56:31.162452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:loss = 46.5171, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-07-20:56:31\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-07-20:56:33\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 104.56, rmse = 10.2255\n",
      "INFO:tensorflow:Validation (step 1): loss = 104.56, global_step = 1, rmse = 10.2255\n",
      "INFO:tensorflow:global_step/sec: 49.9527\n",
      "INFO:tensorflow:loss = 0.0587546, step = 101 (2.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.105\n",
      "INFO:tensorflow:loss = 1.2255, step = 201 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.135\n",
      "INFO:tensorflow:loss = 1.93425, step = 301 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.717\n",
      "INFO:tensorflow:loss = 48.4097, step = 401 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.389\n",
      "INFO:tensorflow:loss = 0.0128693, step = 501 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.525\n",
      "INFO:tensorflow:loss = 2.67007, step = 601 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.17\n",
      "INFO:tensorflow:loss = 6.1624, step = 701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 580.39\n",
      "INFO:tensorflow:loss = 0.894505, step = 801 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.41\n",
      "INFO:tensorflow:loss = 4.1082, step = 901 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.96\n",
      "INFO:tensorflow:loss = 4.39853, step = 1001 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.473\n",
      "INFO:tensorflow:loss = 0.0414448, step = 1101 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.638\n",
      "INFO:tensorflow:loss = 9.03786, step = 1201 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.507\n",
      "INFO:tensorflow:loss = 0.0164026, step = 1301 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.592\n",
      "INFO:tensorflow:loss = 1.54819, step = 1401 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 505.038\n",
      "INFO:tensorflow:loss = 3.25224, step = 1501 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.982\n",
      "INFO:tensorflow:loss = 0.0286783, step = 1601 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.17\n",
      "INFO:tensorflow:loss = 0.13617, step = 1701 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.585\n",
      "INFO:tensorflow:loss = 0.437746, step = 1801 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.979\n",
      "INFO:tensorflow:loss = 8.71206, step = 1901 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.667\n",
      "INFO:tensorflow:loss = 0.574116, step = 2001 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.001\n",
      "INFO:tensorflow:loss = 0.0127322, step = 2101 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.432\n",
      "INFO:tensorflow:loss = 4.93669, step = 2201 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.652\n",
      "INFO:tensorflow:loss = 4.80796, step = 2301 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.484\n",
      "INFO:tensorflow:loss = 4.87082, step = 2401 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.673\n",
      "INFO:tensorflow:loss = 0.0880459, step = 2501 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 567.389\n",
      "INFO:tensorflow:loss = 5.22391, step = 2601 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.376\n",
      "INFO:tensorflow:loss = 0.129045, step = 2701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.521\n",
      "INFO:tensorflow:loss = 0.160012, step = 2801 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.181\n",
      "INFO:tensorflow:loss = 12.0937, step = 2901 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.234\n",
      "INFO:tensorflow:loss = 14.7986, step = 3001 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.916\n",
      "INFO:tensorflow:loss = 3.77539, step = 3101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.152\n",
      "INFO:tensorflow:loss = 4.35685, step = 3201 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.418\n",
      "INFO:tensorflow:loss = 0.326957, step = 3301 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.175\n",
      "INFO:tensorflow:loss = 1.36559, step = 3401 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.774\n",
      "INFO:tensorflow:loss = 1.4876, step = 3501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.532\n",
      "INFO:tensorflow:loss = 0.687843, step = 3601 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.292\n",
      "INFO:tensorflow:loss = 1.84837, step = 3701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 535.553\n",
      "INFO:tensorflow:loss = 34.7207, step = 3801 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.526\n",
      "INFO:tensorflow:loss = 2.23879, step = 3901 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.569\n",
      "INFO:tensorflow:loss = 3.37284, step = 4001 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.329\n",
      "INFO:tensorflow:loss = 0.155274, step = 4101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.569\n",
      "INFO:tensorflow:loss = 3.6533, step = 4201 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.279\n",
      "INFO:tensorflow:loss = 15.9472, step = 4301 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.067\n",
      "INFO:tensorflow:loss = 76.3859, step = 4401 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.521\n",
      "INFO:tensorflow:loss = 2.83802, step = 4501 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.753\n",
      "INFO:tensorflow:loss = 3.21009, step = 4601 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 572.207\n",
      "INFO:tensorflow:loss = 2.74211, step = 4701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 586.789\n",
      "INFO:tensorflow:loss = 3.9564, step = 4801 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.768\n",
      "INFO:tensorflow:loss = 0.429226, step = 4901 (0.174 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 17.7314.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-07-20:56:42\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-07-20:56:44\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 5.93151, rmse = 2.43547\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: abalone_output/export/Servo/1502139404/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local train --package-path trainer \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code on CMLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [abalone_170807_205644] submitted successfully.\n",
      "INFO\t2017-08-07 20:56:46 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2017-08-07 20:56:46 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2017-08-07 20:56:46 +0000\tservice\t\tJob abalone_170807_205644 is queued.\n",
      "INFO\t2017-08-07 20:56:46 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2017-08-07 20:59:15 +0000\tservice\t\tWaiting for TensorFlow to start.\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"worker\", \"index\": 1} --job={\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:25 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"ps\", \"index\": 0} --job={\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:26 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"worker\", \"index\": 3} --job={\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:28 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"master\", \"index\": 0} --job={\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:28 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"ps\", \"index\": 2} --job={\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:30 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-07 21:00:36 +0000\tworker-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:36 +0000\tworker-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:36 +0000\tworker-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:36 +0000\tps-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:36 +0000\tps-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:36 +0000\tps-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:37 +0000\tworker-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:37 +0000\tworker-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:37 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:37 +0000\tps-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:37 +0000\tps-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:38 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tcreating '/tmp/tmpwUfjtPpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-3\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-3\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:38 +0000\tworker-replica-3\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:38 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:38 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:39 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:39 +0000\tmaster-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tcreating '/tmp/tmptGbaKxpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:39 +0000\tps-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-3\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-3\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tcreating '/tmp/tmpXwjhVzpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:39 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:00:40 +0000\tmaster-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:40 +0000\tmaster-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:40 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:40 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:40 +0000\tps-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:40 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:40 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:40 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tcreating '/tmp/tmp9KmnhJpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tcreating '/tmp/tmp2U8v5mpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:41 +0000\tps-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpFKlbzepip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:41 +0000\tworker-replica-3\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:41 +0000\tmaster-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:42 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:42 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:42 +0000\tps-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:42 +0000\tps-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:42 +0000\tworker-replica-3\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:42 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:42 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:42 +0000\tmaster-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:42 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:42 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tcreating '/tmp/tmpk4RWKypip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpJ_bgZYpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:43 +0000\tmaster-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:00:43 +0000\tworker-replica-3\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:00:43 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:43 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tcreating '/tmp/tmp9DNBy3pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tInstalling collected packages: trainer\n",
      "WARNING\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f908957ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-f04601e630-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t2017-08-07 21:00:44.314546: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t2017-08-07 21:00:44.314607: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t2017-08-07 21:00:44.314619: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> localhost:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\t2017-08-07 21:00:44.315461: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-07 21:00:44 +0000\tworker-replica-1\t\tWaiting 4 secs before starting training.\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:44 +0000\tps-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "WARNING\t2017-08-07 21:00:45 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff70763ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-f04601e630-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:45 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t2017-08-07 21:00:45.947890: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t2017-08-07 21:00:45.947944: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t2017-08-07 21:00:45.947964: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:00:45 +0000\tps-replica-0\t\t2017-08-07 21:00:45.949232: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tcreating '/tmp/tmpN0OGb2pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:46 +0000\tps-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa504abee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://master-f04601e630-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tMonitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t2017-08-07 21:00:48.224466: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t2017-08-07 21:00:48.224545: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t2017-08-07 21:00:48.224573: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\t2017-08-07 21:00:48.225139: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0a3b37ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t, '_num_worker_replicas': 4, '_task_id': 3, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-f04601e630-3:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t2017-08-07 21:00:48.378762: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t2017-08-07 21:00:48.378824: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t2017-08-07 21:00:48.378837: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> localhost:2222}\n",
      "INFO\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\tWaiting 14 secs before starting training.\n",
      "ERROR\t2017-08-07 21:00:48 +0000\tworker-replica-3\t\t2017-08-07 21:00:48.379599: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-07 21:00:48 +0000\tmaster-replica-0\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-07 21:00:49 +0000\tworker-replica-1\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"worker\", \"index\": 2} --job={\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:50 +0000\tworker-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "WARNING\t2017-08-07 21:00:50 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:50 +0000\tps-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb94ab3ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-f04601e630-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:50 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t2017-08-07 21:00:50.959490: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t2017-08-07 21:00:50.959545: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> localhost:2222}\n",
      "ERROR\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t2017-08-07 21:00:50.959567: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:00:50 +0000\tps-replica-2\t\t2017-08-07 21:00:50.959954: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-07 21:00:51 +0000\tworker-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:51 +0000\tworker-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tcreating '/tmp/tmptWGzuepip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:52 +0000\tworker-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:53 +0000\tworker-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:53 +0000\tworker-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:53 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tcreating '/tmp/tmpz9nD3vpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:54 +0000\tworker-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"ps\", \"index\": 1} --job={\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:55 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-07 21:00:56 +0000\tps-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:56 +0000\tps-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:56 +0000\tps-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-f04601e630-0:2222\"], \"ps\": [\"ps-f04601e630-0:2222\", \"ps-f04601e630-1:2222\", \"ps-f04601e630-2:2222\"], \"worker\": [\"worker-f04601e630-0:2222\", \"worker-f04601e630-1:2222\", \"worker-f04601e630-2:2222\", \"worker-f04601e630-3:2222\"]} --task={\"type\": \"worker\", \"index\": 0} --job={\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170807_205644\"\n",
      "INFO\t2017-08-07 21:00:56 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:00:57 +0000\tps-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:57 +0000\tps-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:57 +0000\tworker-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-07 21:00:57 +0000\tworker-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:57 +0000\tworker-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:57 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170807_205644/packages/25a3d867393b9ed9d2f58e50f795c0840ed272365d52c2ab488b54acabb9aed7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "WARNING\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f19581fee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-f04601e630-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t2017-08-07 21:00:58.595524: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t2017-08-07 21:00:58.595600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t2017-08-07 21:00:58.595612: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> localhost:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\t2017-08-07 21:00:58.596701: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-2\t\tWaiting 9 secs before starting training.\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tcreating '/tmp/tmpMT0djUpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:58 +0000\tps-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:58 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:59 +0000\tps-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:00:59 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tcreating '/tmp/tmpGuMLUppip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:59 +0000\tworker-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-07 21:00:59 +0000\tps-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:00:59 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:00:59 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:01:00 +0000\tworker-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tcreating '/tmp/tmpCth0Blpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:01:00 +0000\tps-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:01:00 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-07 21:01:00 +0000\tworker-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-07 21:01:00 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-07 21:01:00 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tcreating '/tmp/tmpTh7mMxpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-07 21:01:01 +0000\tworker-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170807_205644\n",
      "INFO\t2017-08-07 21:01:03 +0000\tworker-replica-3\t\tCreate CheckpointSaverHook.\n",
      "WARNING\t2017-08-07 21:01:04 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:01:04 +0000\tps-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe35fe3ee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-f04601e630-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:01:04 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t2017-08-07 21:01:04.620554: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t2017-08-07 21:01:04.621261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> localhost:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t2017-08-07 21:01:04.621299: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> worker-f04601e630-0:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:01:04 +0000\tps-replica-1\t\t2017-08-07 21:01:04.622161: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "ERROR\t2017-08-07 21:01:05 +0000\tworker-replica-1\t\t2017-08-07 21:01:05.169514: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n",
      "WARNING\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170807_205644', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f364debee10>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-f04601e630-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t2017-08-07 21:01:06.057099: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> master-f04601e630-0:2222}\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t2017-08-07 21:01:06.058047: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ps-f04601e630-0:2222, 1 -> ps-f04601e630-1:2222, 2 -> ps-f04601e630-2:2222}\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t2017-08-07 21:01:06.058081: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> worker-f04601e630-1:2222, 2 -> worker-f04601e630-2:2222, 3 -> worker-f04601e630-3:2222}\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t2017-08-07 21:01:06.059512: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\t2017-08-07 21:01:06.440609: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 21396af5f6477151 with config: \n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\tgpu_options {\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\t}\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-3\t\tWaiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, OptimizeLoss/learning_rate, ready: None\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t2017-08-07 21:01:06.606062: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 939674b67f99d101 with config: \n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tgpu_options {\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\t}\n",
      "ERROR\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-07 21:01:06 +0000\tworker-replica-0\t\tWaiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, OptimizeLoss/learning_rate, ready: None\n",
      "ERROR\t2017-08-07 21:01:08 +0000\tmaster-replica-0\t\t2017-08-07 21:01:08.501827: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 1d548eaf7d4b5665 with config: \n",
      "ERROR\t2017-08-07 21:01:08 +0000\tmaster-replica-0\t\tgpu_options {\n",
      "ERROR\t2017-08-07 21:01:08 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-07 21:01:08 +0000\tmaster-replica-0\t\t}\n",
      "ERROR\t2017-08-07 21:01:08 +0000\tmaster-replica-0\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tCreate CheckpointSaverHook.\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\t2017-08-07 21:01:09.131356: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 0d09c44e4946e31d with config: \n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tgpu_options {\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\t}\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tloss = 30.3509, step = 0\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\t2017-08-07 21:01:09.230609: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 4bf70e3f30e4b624 with config: \n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\tgpu_options {\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\t}\n",
      "ERROR\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\tallow_soft_placement: true\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\tloss = 89.1745, step = 16\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tloss = 70.4611, step = 159 (0.364 sec)\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-1\t\tloss = 0.0683735, step = 242 (0.387 sec)\n",
      "INFO\t2017-08-07 21:01:09 +0000\tworker-replica-2\t\tloss = 3.68048, step = 354 (0.370 sec)\n",
      "INFO\t2017-08-07 21:01:10 +0000\tworker-replica-1\t\tloss = 0.314557, step = 454 (0.414 sec)\n",
      "INFO\t2017-08-07 21:01:10 +0000\tworker-replica-2\t\tloss = 0.939316, step = 543 (0.360 sec)\n",
      "INFO\t2017-08-07 21:01:10 +0000\tmaster-replica-0\t\tSaving checkpoints for 605 into gs://smiling-beaming-abalone/abalone_170807_205644/model.ckpt.\n",
      "INFO\t2017-08-07 21:01:10 +0000\tworker-replica-1\t\tloss = 6.9881, step = 662 (0.395 sec)\n",
      "INFO\t2017-08-07 21:01:10 +0000\tworker-replica-2\t\tloss = 3.34186, step = 737 (0.377 sec)\n",
      "INFO\t2017-08-07 21:01:10 +0000\tworker-replica-1\t\tloss = 46.6636, step = 870 (0.381 sec)\n",
      "INFO\t2017-08-07 21:01:11 +0000\tworker-replica-2\t\tloss = 1.10141, step = 930 (0.339 sec)\n",
      "INFO\t2017-08-07 21:01:11 +0000\tworker-replica-1\t\tloss = 0.100077, step = 1069 (0.352 sec)\n",
      "INFO\t2017-08-07 21:01:11 +0000\tworker-replica-2\t\tloss = 1.60373, step = 1132 (0.364 sec)\n",
      "INFO\t2017-08-07 21:01:11 +0000\tworker-replica-1\t\tloss = 18.5044, step = 1273 (0.364 sec)\n",
      "INFO\t2017-08-07 21:01:11 +0000\tworker-replica-2\t\tloss = 12.7679, step = 1331 (0.354 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-1\t\tloss = 0.353643, step = 1476 (0.379 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-2\t\tloss = 1.9595, step = 1530 (0.372 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-1\t\tloss = 1.71072, step = 1669 (0.355 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-2\t\tloss = 1.52862, step = 1732 (0.378 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-1\t\tloss = 113.172, step = 1865 (0.378 sec)\n",
      "INFO\t2017-08-07 21:01:12 +0000\tworker-replica-2\t\tloss = 2.17003, step = 1942 (0.398 sec)\n",
      "INFO\t2017-08-07 21:01:13 +0000\tworker-replica-1\t\tloss = 3.85188, step = 2060 (0.371 sec)\n",
      "INFO\t2017-08-07 21:01:13 +0000\tworker-replica-2\t\tloss = 0.0440029, step = 2145 (0.381 sec)\n",
      "INFO\t2017-08-07 21:01:13 +0000\tworker-replica-1\t\tloss = 1.77117, step = 2258 (0.363 sec)\n",
      "INFO\t2017-08-07 21:01:13 +0000\tworker-replica-2\t\tloss = 0.403024, step = 2345 (0.362 sec)\n",
      "INFO\t2017-08-07 21:01:13 +0000\tworker-replica-1\t\tloss = 2.22774, step = 2458 (0.362 sec)\n",
      "INFO\t2017-08-07 21:01:14 +0000\tworker-replica-2\t\tloss = 3.44366, step = 2538 (0.363 sec)\n",
      "INFO\t2017-08-07 21:01:14 +0000\tworker-replica-1\t\tloss = 2.30522, step = 2660 (0.389 sec)\n",
      "INFO\t2017-08-07 21:01:14 +0000\tworker-replica-2\t\tloss = 13.3858, step = 2743 (0.391 sec)\n",
      "INFO\t2017-08-07 21:01:14 +0000\tworker-replica-1\t\tloss = 24.4163, step = 2861 (0.390 sec)\n",
      "INFO\t2017-08-07 21:01:14 +0000\tworker-replica-2\t\tloss = 11.9589, step = 2938 (0.385 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-1\t\tloss = 3.89984, step = 3074 (0.423 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-2\t\tloss = 11.0821, step = 3124 (0.373 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-1\t\tloss = 1.07167, step = 3293 (0.425 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-2\t\tloss = 17.7756, step = 3308 (0.355 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-2\t\tloss = 84.1262, step = 3500 (0.377 sec)\n",
      "INFO\t2017-08-07 21:01:15 +0000\tworker-replica-1\t\tloss = 6.91123, step = 3502 (0.410 sec)\n",
      "INFO\t2017-08-07 21:01:16 +0000\tworker-replica-2\t\tloss = 0.708684, step = 3682 (0.363 sec)\n",
      "INFO\t2017-08-07 21:01:16 +0000\tworker-replica-1\t\tloss = 6.68691, step = 3723 (0.438 sec)\n",
      "INFO\t2017-08-07 21:01:16 +0000\tworker-replica-2\t\tloss = 1.85428, step = 3877 (0.386 sec)\n",
      "INFO\t2017-08-07 21:01:16 +0000\tworker-replica-1\t\tloss = 0.146808, step = 3928 (0.413 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-2\t\tloss = 0.502425, step = 4073 (0.392 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-1\t\tloss = 4.69436, step = 4129 (0.400 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-2\t\tloss = 0.44955, step = 4270 (0.370 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-1\t\tloss = 0.00233071, step = 4332 (0.377 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-2\t\tloss = 0.167111, step = 4469 (0.357 sec)\n",
      "INFO\t2017-08-07 21:01:17 +0000\tworker-replica-1\t\tloss = 1.5459, step = 4531 (0.352 sec)\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tloss = 1.17729, step = 4679 (0.390 sec)\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tloss = 8.7262, step = 4719 (0.350 sec)\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tloss = 4.51464, step = 4888 (0.387 sec)\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tloss = 5.89343, step = 4913 (0.359 sec)\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tLoss for final step: 0.00703049.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tLoss for final step: 1.74339.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-2\t\tTask completed successfully.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:01:18 +0000\tworker-replica-1\t\tTask completed successfully.\n",
      "INFO\t2017-08-07 21:01:26 +0000\tmaster-replica-0\t\tloss = 3.52301, step = 605\n",
      "INFO\t2017-08-07 21:01:27 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-07-21:01:27\n",
      "INFO\t2017-08-07 21:01:27 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170807_205644/model.ckpt-605\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-07 21:01:28 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-07-21:01:29\n",
      "INFO\t2017-08-07 21:01:29 +0000\tmaster-replica-0\t\tSaving dict for global step 958: global_step = 958, loss = 6.86025, rmse = 2.61921\n",
      "INFO\t2017-08-07 21:01:32 +0000\tmaster-replica-0\t\tValidation (step 593): loss = 6.86025, global_step = 958, rmse = 2.61921\n",
      "INFO\t2017-08-07 21:01:32 +0000\tmaster-replica-0\t\tglobal_step/sec: 715.342\n",
      "INFO\t2017-08-07 21:01:32 +0000\tmaster-replica-0\t\tSaving checkpoints for 5003 into gs://smiling-beaming-abalone/abalone_170807_205644/model.ckpt.\n",
      "INFO\t2017-08-07 21:01:44 +0000\tmaster-replica-0\t\tLoss for final step: 2.58251.\n",
      "INFO\t2017-08-07 21:01:44 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-07-21:01:44\n",
      "INFO\t2017-08-07 21:01:45 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170807_205644/model.ckpt-5003\n",
      "ERROR\t2017-08-07 21:01:46 +0000\tworker-replica-3\t\t2017-08-07 21:01:46.508728: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:01:46 +0000\tworker-replica-3\t\t2017-08-07 21:01:46.508814: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:01:46 +0000\tworker-replica-0\t\t2017-08-07 21:01:46.654076: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:01:46 +0000\tworker-replica-0\t\t2017-08-07 21:01:46.654163: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-07 21:01:49 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-07-21:01:50\n",
      "INFO\t2017-08-07 21:01:50 +0000\tmaster-replica-0\t\tSaving dict for global step 5003: global_step = 5003, loss = 6.96539, rmse = 2.6392\n",
      "INFO\t2017-08-07 21:01:51 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170807_205644/model.ckpt-5003\n",
      "INFO\t2017-08-07 21:01:54 +0000\tmaster-replica-0\t\tAssets added to graph.\n",
      "INFO\t2017-08-07 21:01:54 +0000\tmaster-replica-0\t\tNo assets to write.\n",
      "ERROR\t2017-08-07 21:01:56 +0000\tworker-replica-3\t\t2017-08-07 21:01:56.508992: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:01:56 +0000\tworker-replica-3\t\t2017-08-07 21:01:56.509093: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:01:56 +0000\tworker-replica-0\t\t2017-08-07 21:01:56.654309: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:01:56 +0000\tworker-replica-0\t\t2017-08-07 21:01:56.654427: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "INFO\t2017-08-07 21:02:01 +0000\tmaster-replica-0\t\tSavedModel written to: gs://smiling-beaming-abalone/abalone_170807_205644/export/Servo/1502139711/saved_model.pb\n",
      "INFO\t2017-08-07 21:02:02 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:02 +0000\tmaster-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:02:02 +0000\tmaster-replica-0\t\tTask completed successfully.\n",
      "ERROR\t2017-08-07 21:02:06 +0000\tworker-replica-3\t\t2017-08-07 21:02:06.509228: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:06 +0000\tworker-replica-3\t\t2017-08-07 21:02:06.509348: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:06 +0000\tworker-replica-0\t\t2017-08-07 21:02:06.654605: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:06 +0000\tworker-replica-0\t\t2017-08-07 21:02:06.654686: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:16 +0000\tworker-replica-3\t\t2017-08-07 21:02:16.509478: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:16 +0000\tworker-replica-3\t\t2017-08-07 21:02:16.509556: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:16 +0000\tworker-replica-0\t\t2017-08-07 21:02:16.654824: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:16 +0000\tworker-replica-0\t\t2017-08-07 21:02:16.654914: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "INFO\t2017-08-07 21:02:19 +0000\tservice\t\tTearing down TensorFlow.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-0\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-1\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-2\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tps-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-0\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-3\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-3\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-07 21:02:19 +0000\tworker-replica-3\t\tClean up finished.\n",
      "ERROR\t2017-08-07 21:02:26 +0000\tworker-replica-3\t\t2017-08-07 21:02:26.509687: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:26 +0000\tworker-replica-3\t\t2017-08-07 21:02:26.509781: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:26 +0000\tworker-replica-0\t\t2017-08-07 21:02:26.655047: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:26 +0000\tworker-replica-0\t\t2017-08-07 21:02:26.655127: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:36 +0000\tworker-replica-3\t\t2017-08-07 21:02:36.509907: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:36 +0000\tworker-replica-3\t\t2017-08-07 21:02:36.509991: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "ERROR\t2017-08-07 21:02:36 +0000\tworker-replica-0\t\t2017-08-07 21:02:36.655278: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n",
      "ERROR\t2017-08-07 21:02:36 +0000\tworker-replica-0\t\t2017-08-07 21:02:36.655428: I tensorflow/core/distributed_runtime/master.cc:209] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2\n",
      "INFO\t2017-08-07 21:03:55 +0000\tservice\t\tFinished tearing down TensorFlow.\n",
      "INFO\t2017-08-07 21:03:55 +0000\tservice\t\tJob completed successfully.\n",
      "endTime: '2017-08-07T21:02:19'\n",
      "jobId: abalone_170807_205644\n",
      "startTime: '2017-08-07T21:01:18'\n",
      "state: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.2 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path trainer \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
