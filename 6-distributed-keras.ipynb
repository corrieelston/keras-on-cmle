{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our code, locally to begin with, using CMLE.\n",
    "\n",
    "Here are the changes we need to make convert our code to a python script file in folder called trainer and convert that folder into a package by adding an __init__.py script file.\n",
    "\n",
    "After that we'll test the code from the command line.\n",
    "\n",
    "Then we'll run the full, distributed job on CMLE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some housekeeping, we need to delete the output folder we use locally between runs (in case you run this notebook multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf trainer && rm -rf abalone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import six\n",
    "from six.moves import urllib\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = ['length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n",
    "               'viscera_weight', 'shell_weight', 'num_rings']\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "PREDICTED_COLUMN = 'num_rings'\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('length'),\n",
    "    tf.feature_column.numeric_column('diameter'),\n",
    "    tf.feature_column.numeric_column('height'),\n",
    "    tf.feature_column.numeric_column('whole_weight'),\n",
    "    tf.feature_column.numeric_column('shucked_weight'),\n",
    "    tf.feature_column.numeric_column('viscera_weight'),\n",
    "    tf.feature_column.numeric_column('shell_weight'),\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - {PREDICTED_COLUMN}\n",
    "\n",
    "def parse_csv(rows_string_tensor):\n",
    "  columns = tf.decode_csv(rows_string_tensor, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "  features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "  for col in UNUSED_COLUMNS:\n",
    "    features.pop(col)\n",
    "\n",
    "  for key, value in six.iteritems(features):\n",
    "    features[key] = tf.expand_dims(features[key], -1)\n",
    "  return features\n",
    "\n",
    "def generate_input_fn(filenames,\n",
    "                      num_epochs=None,\n",
    "                      shuffle=True,\n",
    "                      skip_header_lines=0,\n",
    "                      batch_size=64):\n",
    "  \n",
    "  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=shuffle)\n",
    "  reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
    "\n",
    "  _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "  features = parse_csv(rows)\n",
    "\n",
    "  if shuffle:\n",
    "    features = tf.train.shuffle_batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        min_after_dequeue=2 * batch_size + 1,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "  else:\n",
    "    features = tf.train.batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "\n",
    "  return features, features.pop(PREDICTED_COLUMN)\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense\n",
    "\n",
    "def generate_model_fn(learning_rate):\n",
    "    \n",
    "    def _model_fn(mode, features, labels):\n",
    "\n",
    "        (length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight) = INPUT_COLUMNS\n",
    "\n",
    "        transformed_columns = [\n",
    "            length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight\n",
    "        ]\n",
    "\n",
    "        inputs = tf.feature_column.input_layer(features, transformed_columns)\n",
    "\n",
    "        first_hidden_layer = Dense(10, activation='relu')(inputs)\n",
    "        second_hidden_layer = Dense(10, activation='relu')(first_hidden_layer)\n",
    "        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n",
    "\n",
    "        if mode in (Modes.PREDICT, Modes.EVAL):\n",
    "          predictions = tf.reshape(output_layer, [-1])\n",
    "          predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "        if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "          loss = tf.losses.mean_squared_error(labels, output_layer)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "          train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "        \n",
    "        if mode == Modes.TRAIN:\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        if mode == Modes.EVAL:\n",
    "          eval_metric_ops = {\n",
    "              \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "                  tf.cast(labels, tf.float32), predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "        if mode == Modes.PREDICT:\n",
    "          export_outputs = {\n",
    "              'prediction': tf.estimator.export.RegressionOutput(predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions_dict, export_outputs=export_outputs)\n",
    "    \n",
    "    return _model_fn\n",
    "  \n",
    "def generate_experiment_fn(**experiment_args):  \n",
    "  \n",
    "  def _experiment_fn(run_config, hparams):\n",
    "\n",
    "    train_input = lambda: generate_input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size,\n",
    "    )\n",
    "\n",
    "    test_input = lambda: generate_input_fn(\n",
    "      hparams.eval_files,\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        tf.estimator.Estimator(\n",
    "            generate_model_fn(learning_rate=hparams.learning_rate),\n",
    "            config=run_config\n",
    "        ),\n",
    "        train_input_fn=train_input,\n",
    "        eval_input_fn=test_input,\n",
    "        **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn\n",
    "\n",
    "def example_serving_input_fn():\n",
    "  \"\"\"Build the serving inputs.\"\"\"\n",
    "  example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "  )\n",
    "  features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n",
    "  )\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS or local paths to training data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate for the optimizer',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=[\n",
    "            'DEBUG',\n",
    "            'ERROR',\n",
    "            'FATAL',\n",
    "            'INFO',\n",
    "            'WARN'\n",
    "        ],\n",
    "        default='INFO',\n",
    "        help='Set logging verbosity'\n",
    "    )\n",
    "    # Experiment arguments\n",
    "    parser.add_argument(\n",
    "        '--eval-delay-secs',\n",
    "        help='How long to wait before running first evaluation',\n",
    "        default=10,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min-eval-frequency',\n",
    "        help='Minimum number of training steps between evaluations',\n",
    "        default=1,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help=\"\"\"\\\n",
    "        Number of steps to run evalution for at each checkpoint.\n",
    "        If unspecified will run until the input from --eval-files is exhausted\n",
    "        \"\"\",\n",
    "        default=None,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    '''args = parser.parse_args([\n",
    "        '--train-files', 'gs://smiling-beaming-abalone/abalone_train.csv',\n",
    "        '--eval-files', 'gs://smiling-beaming-abalone/abalone_test.csv',\n",
    "        '--job-dir', 'abalone_output',\n",
    "        '--train-steps', '5000',\n",
    "        '--eval-steps', '100'\n",
    "      ])'''\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set python level verbosity\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    # Set C++ Graph Execution level verbosity\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "        tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "    # Run the training job\n",
    "    # learn_runner pulls configuration information from environment\n",
    "    # variables using tf.learn.RunConfig and uses this configuration\n",
    "    # to conditionally execute Experiment, or param server code\n",
    "    learn_runner.run(\n",
    "        generate_experiment_fn(\n",
    "            min_eval_frequency=args.min_eval_frequency,\n",
    "            eval_delay_secs=args.eval_delay_secs,\n",
    "            train_steps=args.train_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "                example_serving_input_fn,\n",
    "                exports_to_keep=1\n",
    "            )]\n",
    "        ),\n",
    "        run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n",
    "        hparams=hparam.HParams(**args.__dict__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'abalone_output', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2350dba710>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_session_config': None}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:loss = 166.551, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-02-20:10:02\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-02-20:10:03\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 90.3884, rmse = 9.50728\n",
      "INFO:tensorflow:Validation (step 1): loss = 90.3884, global_step = 1, rmse = 9.50728\n",
      "INFO:tensorflow:global_step/sec: 50.1412\n",
      "INFO:tensorflow:loss = 13.6493, step = 101 (1.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 538.355\n",
      "INFO:tensorflow:loss = 0.000520208, step = 201 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.419\n",
      "INFO:tensorflow:loss = 6.33799, step = 301 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.087\n",
      "INFO:tensorflow:loss = 0.996534, step = 401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.277\n",
      "INFO:tensorflow:loss = 1.30767, step = 501 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.297\n",
      "INFO:tensorflow:loss = 5.87026, step = 601 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.309\n",
      "INFO:tensorflow:loss = 0.500734, step = 701 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.566\n",
      "INFO:tensorflow:loss = 1.82504, step = 801 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.491\n",
      "INFO:tensorflow:loss = 0.000113578, step = 901 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.794\n",
      "INFO:tensorflow:loss = 4.81791, step = 1001 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.8\n",
      "INFO:tensorflow:loss = 1.53059, step = 1101 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.554\n",
      "INFO:tensorflow:loss = 3.67981, step = 1201 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.956\n",
      "INFO:tensorflow:loss = 2.64114, step = 1301 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.305\n",
      "INFO:tensorflow:loss = 88.0327, step = 1401 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.322\n",
      "INFO:tensorflow:loss = 26.2771, step = 1501 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.842\n",
      "INFO:tensorflow:loss = 6.51291, step = 1601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.894\n",
      "INFO:tensorflow:loss = 1.51841, step = 1701 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.414\n",
      "INFO:tensorflow:loss = 2.28144, step = 1801 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.666\n",
      "INFO:tensorflow:loss = 0.169888, step = 1901 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.858\n",
      "INFO:tensorflow:loss = 5.3267, step = 2001 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 535.857\n",
      "INFO:tensorflow:loss = 0.136355, step = 2101 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.296\n",
      "INFO:tensorflow:loss = 0.138058, step = 2201 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 547.549\n",
      "INFO:tensorflow:loss = 17.2621, step = 2301 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.957\n",
      "INFO:tensorflow:loss = 2.84261, step = 2401 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.36\n",
      "INFO:tensorflow:loss = 1.28084, step = 2501 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.291\n",
      "INFO:tensorflow:loss = 0.273049, step = 2601 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.35\n",
      "INFO:tensorflow:loss = 4.52193, step = 2701 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.982\n",
      "INFO:tensorflow:loss = 0.00371098, step = 2801 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 538.175\n",
      "INFO:tensorflow:loss = 0.833024, step = 2901 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.908\n",
      "INFO:tensorflow:loss = 1.80437, step = 3001 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.953\n",
      "INFO:tensorflow:loss = 7.53222, step = 3101 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 542.944\n",
      "INFO:tensorflow:loss = 1.78169, step = 3201 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 538.677\n",
      "INFO:tensorflow:loss = 15.6069, step = 3301 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.048\n",
      "INFO:tensorflow:loss = 0.962973, step = 3401 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.658\n",
      "INFO:tensorflow:loss = 5.23158, step = 3501 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 533.177\n",
      "INFO:tensorflow:loss = 0.900415, step = 3601 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.888\n",
      "INFO:tensorflow:loss = 1.11406, step = 3701 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.829\n",
      "INFO:tensorflow:loss = 0.0392597, step = 3801 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.431\n",
      "INFO:tensorflow:loss = 14.9223, step = 3901 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.444\n",
      "INFO:tensorflow:loss = 10.4056, step = 4001 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.66\n",
      "INFO:tensorflow:loss = 0.184491, step = 4101 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.647\n",
      "INFO:tensorflow:loss = 6.39014, step = 4201 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 556.291\n",
      "INFO:tensorflow:loss = 0.523914, step = 4301 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.246\n",
      "INFO:tensorflow:loss = 0.0295709, step = 4401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.159\n",
      "INFO:tensorflow:loss = 1.93395, step = 4501 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.198\n",
      "INFO:tensorflow:loss = 2.33999, step = 4601 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.215\n",
      "INFO:tensorflow:loss = 0.463745, step = 4701 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.451\n",
      "INFO:tensorflow:loss = 1.49808, step = 4801 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 512.839\n",
      "INFO:tensorflow:loss = 33.5612, step = 4901 (0.195 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.3131.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-02-20:10:13\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-02-20:10:15\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 6.16375, rmse = 2.48269\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: abalone_output/export/Servo/1501704615/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local train --package-path trainer \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code on CMLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [abalone_170802_201015] submitted successfully.\n",
      "INFO\t2017-08-02 20:10:17 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2017-08-02 20:10:17 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2017-08-02 20:10:17 +0000\tservice\t\tJob abalone_170802_201015 is queued.\n",
      "INFO\t2017-08-02 20:10:17 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2017-08-02 20:13:22 +0000\tservice\t\tWaiting for TensorFlow to start.\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"ps\", \"index\": 2} --job={\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:17 +0000\tps-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"ps\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:18 +0000\tps-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"worker\", \"index\": 2} --job={\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:18 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"worker\", \"index\": 1} --job={\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"master\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tworker-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:19 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tcreating '/tmp/tmpFvzgMTpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"worker\", \"index\": 3} --job={\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"worker\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-3\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tmaster-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tmaster-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tcreating '/tmp/tmp4H24Xjpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:20 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:20 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tcreating '/tmp/tmpTP7BHLpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-3\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tcreating '/tmp/tmpLG4SD8pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpdonh9cpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tcreating '/tmp/tmpowkcOopip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:21 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tcreating '/tmp/tmpAMS9Mvpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tcreating '/tmp/tmpXL87lspip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tcreating '/tmp/tmpdYHYnWpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tcreating '/tmp/tmpvntQsnpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-3\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tcreating '/tmp/tmpZMPI4vpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:22 +0000\tps-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:22 +0000\tworker-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpc1kO2Npip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:22 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:23 +0000\tmaster-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tcreating '/tmp/tmp8rjSjopip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tcreating '/tmp/tmpc1cbddpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "INFO\t2017-08-02 20:14:23 +0000\tworker-replica-3\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcd41b08290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-d990470f27-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f76faf88290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-d990470f27-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-2\t\tWaiting 9 secs before starting training.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa981488290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-d990470f27-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tworker-replica-1\t\tWaiting 4 secs before starting training.\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe487f88290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:26 +0000\tps-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-d990470f27-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:26 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01cbdc4290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://master-d990470f27-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tMonitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2335808290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-d990470f27-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6b22cc8290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\t, '_num_worker_replicas': 4, '_task_id': 3, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-d990470f27-3:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-3\t\tWaiting 14 secs before starting training.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tmaster-replica-0\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 20:14:27 +0000\tworker-replica-0\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 20:14:31 +0000\tworker-replica-1\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 20:14:36 +0000\tworker-replica-2\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 20:14:42 +0000\tworker-replica-3\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-d990470f27-0:2222\"], \"ps\": [\"ps-d990470f27-0:2222\", \"ps-d990470f27-1:2222\", \"ps-d990470f27-2:2222\"], \"worker\": [\"worker-d990470f27-0:2222\", \"worker-d990470f27-1:2222\", \"worker-d990470f27-2:2222\", \"worker-d990470f27-3:2222\"]} --task={\"type\": \"ps\", \"index\": 1} --job={\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_201015\"\n",
      "INFO\t2017-08-02 20:14:48 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-02 20:14:49 +0000\tps-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 20:14:49 +0000\tps-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:49 +0000\tps-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:50 +0000\tps-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_201015/packages/2185136e44bd62ef2a1543173dca2236c8b028ea7621a46f6bf3fa8269b5aca0/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:50 +0000\tps-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:50 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tcreating '/tmp/tmp16twl6pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 20:14:51 +0000\tps-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tcreating '/tmp/tmpiiP0mDpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 20:14:52 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 20:14:53 +0000\tps-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_201015\n",
      "WARNING\t2017-08-02 20:14:56 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:56 +0000\tps-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_201015', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3754588290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 20:14:56 +0000\tps-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 20:14:56 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-02 20:14:56 +0000\tps-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-d990470f27-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 20:14:56 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 20:14:59 +0000\tworker-replica-0\t\tWaiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, OptimizeLoss/learning_rate, ready: None\n",
      "INFO\t2017-08-02 20:14:59 +0000\tworker-replica-3\t\tWaiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, OptimizeLoss/learning_rate, ready: None\n",
      "INFO\t2017-08-02 20:15:01 +0000\tmaster-replica-0\t\tSaving checkpoints for 0 into gs://smiling-beaming-abalone/abalone_170802_201015/model.ckpt.\n",
      "INFO\t2017-08-02 20:15:06 +0000\tworker-replica-2\t\tloss = 16.9491, step = 201 (0.297 sec)\n",
      "INFO\t2017-08-02 20:15:07 +0000\tworker-replica-2\t\tloss = 11.2404, step = 301 (0.300 sec)\n",
      "INFO\t2017-08-02 20:15:07 +0000\tworker-replica-2\t\tloss = 0.261261, step = 401 (0.296 sec)\n",
      "INFO\t2017-08-02 20:15:07 +0000\tworker-replica-2\t\tloss = 0.0161041, step = 501 (0.328 sec)\n",
      "INFO\t2017-08-02 20:15:08 +0000\tworker-replica-2\t\tloss = 4.16442, step = 601 (0.311 sec)\n",
      "INFO\t2017-08-02 20:15:08 +0000\tworker-replica-2\t\tloss = 0.114162, step = 701 (0.328 sec)\n",
      "INFO\t2017-08-02 20:15:08 +0000\tworker-replica-2\t\tloss = 6.27103, step = 801 (0.306 sec)\n",
      "INFO\t2017-08-02 20:15:09 +0000\tworker-replica-2\t\tloss = 34.1329, step = 901 (0.302 sec)\n",
      "INFO\t2017-08-02 20:15:09 +0000\tworker-replica-2\t\tloss = 17.1882, step = 1001 (0.288 sec)\n",
      "INFO\t2017-08-02 20:15:09 +0000\tworker-replica-2\t\tloss = 12.0296, step = 1101 (0.306 sec)\n",
      "INFO\t2017-08-02 20:15:10 +0000\tworker-replica-2\t\tloss = 2.68245, step = 1301 (0.323 sec)\n",
      "INFO\t2017-08-02 20:15:10 +0000\tworker-replica-2\t\tloss = 0.171678, step = 1401 (0.306 sec)\n",
      "INFO\t2017-08-02 20:15:10 +0000\tworker-replica-2\t\tloss = 10.9046, step = 1501 (0.298 sec)\n",
      "INFO\t2017-08-02 20:15:11 +0000\tworker-replica-2\t\tloss = 7.11127, step = 1601 (0.301 sec)\n",
      "INFO\t2017-08-02 20:15:11 +0000\tworker-replica-2\t\tloss = 5.5831, step = 1701 (0.305 sec)\n",
      "INFO\t2017-08-02 20:15:11 +0000\tworker-replica-2\t\tloss = 22.4296, step = 1801 (0.309 sec)\n",
      "INFO\t2017-08-02 20:15:12 +0000\tworker-replica-2\t\tloss = 1.10118, step = 2101 (0.305 sec)\n",
      "INFO\t2017-08-02 20:15:14 +0000\tworker-replica-2\t\tloss = 0.680815, step = 2601 (0.300 sec)\n",
      "INFO\t2017-08-02 20:15:14 +0000\tworker-replica-2\t\tloss = 1.83132, step = 2701 (0.298 sec)\n",
      "INFO\t2017-08-02 20:15:14 +0000\tworker-replica-2\t\tloss = 5.61005, step = 2801 (0.304 sec)\n",
      "INFO\t2017-08-02 20:15:15 +0000\tworker-replica-2\t\tloss = 0.000574476, step = 2901 (0.316 sec)\n",
      "INFO\t2017-08-02 20:15:15 +0000\tworker-replica-2\t\tloss = 2.21273, step = 3001 (0.314 sec)\n",
      "INFO\t2017-08-02 20:15:15 +0000\tworker-replica-2\t\tloss = 3.60085, step = 3101 (0.306 sec)\n",
      "INFO\t2017-08-02 20:15:16 +0000\tworker-replica-2\t\tloss = 5.26001, step = 3201 (0.300 sec)\n",
      "INFO\t2017-08-02 20:15:16 +0000\tworker-replica-2\t\tloss = 8.67191, step = 3301 (0.309 sec)\n",
      "INFO\t2017-08-02 20:15:16 +0000\tworker-replica-2\t\tloss = 10.683, step = 3401 (0.367 sec)\n",
      "INFO\t2017-08-02 20:15:17 +0000\tworker-replica-2\t\tloss = 3.00102, step = 3501 (0.323 sec)\n",
      "INFO\t2017-08-02 20:15:17 +0000\tworker-replica-2\t\tloss = 3.90019, step = 3601 (0.356 sec)\n",
      "INFO\t2017-08-02 20:15:17 +0000\tworker-replica-2\t\tloss = 2.5529, step = 3701 (0.307 sec)\n",
      "INFO\t2017-08-02 20:15:18 +0000\tmaster-replica-0\t\tloss = 98.8988, step = 0\n",
      "INFO\t2017-08-02 20:15:18 +0000\tworker-replica-2\t\tloss = 0.0140888, step = 3801 (0.308 sec)\n",
      "INFO\t2017-08-02 20:15:18 +0000\tworker-replica-2\t\tloss = 0.251081, step = 3901 (0.323 sec)\n",
      "INFO\t2017-08-02 20:15:18 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-02-20:15:18\n",
      "INFO\t2017-08-02 20:15:18 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_201015/model.ckpt-0\n",
      "INFO\t2017-08-02 20:15:18 +0000\tworker-replica-2\t\tloss = 0.0258563, step = 4001 (0.317 sec)\n",
      "INFO\t2017-08-02 20:15:19 +0000\tworker-replica-2\t\tloss = 29.8638, step = 4101 (0.329 sec)\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tworker-replica-2\t\tloss = 0.191061, step = 4201 (0.318 sec)\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tworker-replica-2\t\tloss = 1.63854, step = 4301 (0.307 sec)\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-02 20:15:19 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tworker-replica-2\t\tloss = 0.805073, step = 4401 (0.299 sec)\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-02-20:15:20\n",
      "INFO\t2017-08-02 20:15:20 +0000\tmaster-replica-0\t\tSaving dict for global step 1: global_step = 1, loss = 111.304, rmse = 10.5501\n",
      "INFO\t2017-08-02 20:15:20 +0000\tworker-replica-2\t\tloss = 0.700337, step = 4501 (0.291 sec)\n",
      "INFO\t2017-08-02 20:15:20 +0000\tworker-replica-2\t\tloss = 1.77977, step = 4601 (0.316 sec)\n",
      "INFO\t2017-08-02 20:15:21 +0000\tworker-replica-2\t\tloss = 0.166782, step = 4701 (0.332 sec)\n",
      "INFO\t2017-08-02 20:15:21 +0000\tworker-replica-2\t\tloss = 0.220982, step = 4801 (0.323 sec)\n",
      "INFO\t2017-08-02 20:15:21 +0000\tworker-replica-2\t\tloss = 0.123761, step = 4901 (0.319 sec)\n",
      "INFO\t2017-08-02 20:15:21 +0000\tworker-replica-2\t\tLoss for final step: 2.95158.\n",
      "INFO\t2017-08-02 20:15:22 +0000\tworker-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:22 +0000\tworker-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:22 +0000\tworker-replica-2\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 20:15:23 +0000\tmaster-replica-0\t\tValidation (step 1): loss = 111.304, global_step = 1, rmse = 10.5501\n",
      "INFO\t2017-08-02 20:15:23 +0000\tmaster-replica-0\t\tglobal_step/sec: 994.032\n",
      "INFO\t2017-08-02 20:15:23 +0000\tmaster-replica-0\t\tSaving checkpoints for 5002 into gs://smiling-beaming-abalone/abalone_170802_201015/model.ckpt.\n",
      "INFO\t2017-08-02 20:15:34 +0000\tmaster-replica-0\t\tLoss for final step: 13.869.\n",
      "INFO\t2017-08-02 20:15:34 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-02-20:15:34\n",
      "INFO\t2017-08-02 20:15:34 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_201015/model.ckpt-5002\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-02 20:15:35 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-02-20:15:36\n",
      "INFO\t2017-08-02 20:15:36 +0000\tmaster-replica-0\t\tSaving dict for global step 5002: global_step = 5002, loss = 5.90221, rmse = 2.42945\n",
      "INFO\t2017-08-02 20:15:37 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_201015/model.ckpt-5002\n",
      "INFO\t2017-08-02 20:15:39 +0000\tmaster-replica-0\t\tAssets added to graph.\n",
      "INFO\t2017-08-02 20:15:39 +0000\tmaster-replica-0\t\tNo assets to write.\n",
      "INFO\t2017-08-02 20:15:45 +0000\tmaster-replica-0\t\tSavedModel written to: gs://smiling-beaming-abalone/abalone_170802_201015/export/Servo/1501704937/saved_model.pb\n",
      "INFO\t2017-08-02 20:15:45 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:45 +0000\tmaster-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:45 +0000\tmaster-replica-0\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tservice\t\tTearing down TensorFlow.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-0\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-1\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-0\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-2\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tps-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-1\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-3\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-3\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 20:15:54 +0000\tworker-replica-3\t\tClean up finished.\n",
      "INFO\t2017-08-02 20:16:44 +0000\tservice\t\tFinished tearing down TensorFlow.\n",
      "INFO\t2017-08-02 20:16:44 +0000\tservice\t\tJob completed successfully.\n",
      "endTime: '2017-08-02T20:15:54'\n",
      "jobId: abalone_170802_201015\n",
      "startTime: '2017-08-02T20:14:53'\n",
      "state: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.2 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path trainer \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
