{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our code, locally to begin with, using CMLE.\n",
    "\n",
    "Here are the changes we need to make convert our code to a python script file in folder called trainer and convert that folder into a package by adding an __init__.py script file.\n",
    "\n",
    "After that we'll test the code from the command line.\n",
    "\n",
    "Then we'll run the full, distributed job on CMLE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some housekeeping, we need to delete the output folder we use locally between runs (in case you run this notebook multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf trainer && rm -rf abalone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import six\n",
    "from six.moves import urllib\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = ['length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n",
    "               'viscera_weight', 'shell_weight', 'num_rings']\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "PREDICTED_COLUMN = 'num_rings'\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('length'),\n",
    "    tf.feature_column.numeric_column('diameter'),\n",
    "    tf.feature_column.numeric_column('height'),\n",
    "    tf.feature_column.numeric_column('whole_weight'),\n",
    "    tf.feature_column.numeric_column('shucked_weight'),\n",
    "    tf.feature_column.numeric_column('viscera_weight'),\n",
    "    tf.feature_column.numeric_column('shell_weight'),\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - {PREDICTED_COLUMN}\n",
    "\n",
    "def parse_csv(rows_string_tensor):\n",
    "  columns = tf.decode_csv(rows_string_tensor, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "  features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "  for col in UNUSED_COLUMNS:\n",
    "    features.pop(col)\n",
    "\n",
    "  for key, value in six.iteritems(features):\n",
    "    features[key] = tf.expand_dims(features[key], -1)\n",
    "  return features\n",
    "\n",
    "def generate_input_fn(filenames,\n",
    "                      num_epochs=None,\n",
    "                      shuffle=True,\n",
    "                      skip_header_lines=0,\n",
    "                      batch_size=64):\n",
    "  \n",
    "  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=shuffle)\n",
    "  reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
    "\n",
    "  _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "  features = parse_csv(rows)\n",
    "\n",
    "  if shuffle:\n",
    "    features = tf.train.shuffle_batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        min_after_dequeue=2 * batch_size + 1,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "  else:\n",
    "    features = tf.train.batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "\n",
    "  return features, features.pop(PREDICTED_COLUMN)\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense\n",
    "\n",
    "def generate_model_fn(learning_rate):\n",
    "    \n",
    "    def _model_fn(mode, features, labels):\n",
    "\n",
    "        (length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight) = INPUT_COLUMNS\n",
    "\n",
    "        transformed_columns = [\n",
    "            length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight\n",
    "        ]\n",
    "\n",
    "        inputs = tf.feature_column.input_layer(features, transformed_columns)\n",
    "\n",
    "        first_hidden_layer = Dense(10, activation='relu')(inputs)\n",
    "        second_hidden_layer = Dense(10, activation='relu')(first_hidden_layer)\n",
    "        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n",
    "\n",
    "        if mode in (Modes.PREDICT, Modes.EVAL):\n",
    "          predictions = tf.reshape(output_layer, [-1])\n",
    "          predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "        if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "          loss = tf.losses.mean_squared_error(labels, output_layer)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "          train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "        \n",
    "        if mode == Modes.TRAIN:\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        if mode == Modes.EVAL:\n",
    "          eval_metric_ops = {\n",
    "              \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "                  tf.cast(labels, tf.float32), predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "        if mode == Modes.PREDICT:\n",
    "          export_outputs = {\n",
    "              'prediction': tf.estimator.export.RegressionOutput(predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions_dict, export_outputs=export_outputs)\n",
    "    \n",
    "    return _model_fn\n",
    "  \n",
    "def generate_experiment_fn(**experiment_args):  \n",
    "  \n",
    "  def _experiment_fn(run_config, hparams):\n",
    "\n",
    "    train_input = lambda: generate_input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size,\n",
    "    )\n",
    "\n",
    "    test_input = lambda: generate_input_fn(\n",
    "      hparams.eval_files,\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        tf.estimator.Estimator(\n",
    "            generate_model_fn(learning_rate=hparams.learning_rate),\n",
    "            config=run_config\n",
    "        ),\n",
    "        train_input_fn=train_input,\n",
    "        eval_input_fn=test_input,\n",
    "        **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn\n",
    "\n",
    "def example_serving_input_fn():\n",
    "  \"\"\"Build the serving inputs.\"\"\"\n",
    "  example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "  )\n",
    "  features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n",
    "  )\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS or local paths to training data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate for the optimizer',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=[\n",
    "            'DEBUG',\n",
    "            'ERROR',\n",
    "            'FATAL',\n",
    "            'INFO',\n",
    "            'WARN'\n",
    "        ],\n",
    "        default='INFO',\n",
    "        help='Set logging verbosity'\n",
    "    )\n",
    "    # Experiment arguments\n",
    "    parser.add_argument(\n",
    "        '--eval-delay-secs',\n",
    "        help='How long to wait before running first evaluation',\n",
    "        default=10,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min-eval-frequency',\n",
    "        help='Minimum number of training steps between evaluations',\n",
    "        default=1,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help=\"\"\"\\\n",
    "        Number of steps to run evalution for at each checkpoint.\n",
    "        If unspecified will run until the input from --eval-files is exhausted\n",
    "        \"\"\",\n",
    "        default=None,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    '''args = parser.parse_args([\n",
    "        '--train-files', 'gs://smiling-beaming-abalone/abalone_train.csv',\n",
    "        '--eval-files', 'gs://smiling-beaming-abalone/abalone_test.csv',\n",
    "        '--job-dir', 'abalone_output',\n",
    "        '--train-steps', '5000',\n",
    "        '--eval-steps', '100'\n",
    "      ])'''\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set python level verbosity\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    # Set C++ Graph Execution level verbosity\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "        tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "    # Run the training job\n",
    "    # learn_runner pulls configuration information from environment\n",
    "    # variables using tf.learn.RunConfig and uses this configuration\n",
    "    # to conditionally execute Experiment, or param server code\n",
    "    learn_runner.run(\n",
    "        generate_experiment_fn(\n",
    "            min_eval_frequency=args.min_eval_frequency,\n",
    "            eval_delay_secs=args.eval_delay_secs,\n",
    "            train_steps=args.train_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "                example_serving_input_fn,\n",
    "                exports_to_keep=1\n",
    "            )]\n",
    "        ),\n",
    "        run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n",
    "        hparams=hparam.HParams(**args.__dict__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'abalone_output', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fad8b77a710>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_session_config': None}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:loss = 37.1836, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-02-18:42:43\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-02-18:42:45\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 113.283, rmse = 10.6435\n",
      "INFO:tensorflow:Validation (step 1): loss = 113.283, global_step = 1, rmse = 10.6435\n",
      "INFO:tensorflow:global_step/sec: 46.9591\n",
      "INFO:tensorflow:loss = 0.0627711, step = 101 (2.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.034\n",
      "INFO:tensorflow:loss = 0.220904, step = 201 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.79\n",
      "INFO:tensorflow:loss = 4.11874, step = 301 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.515\n",
      "INFO:tensorflow:loss = 3.9867, step = 401 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.812\n",
      "INFO:tensorflow:loss = 2.37013, step = 501 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 577.327\n",
      "INFO:tensorflow:loss = 1.38279, step = 601 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.503\n",
      "INFO:tensorflow:loss = 0.341312, step = 701 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.162\n",
      "INFO:tensorflow:loss = 3.94243, step = 801 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.645\n",
      "INFO:tensorflow:loss = 13.5552, step = 901 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.163\n",
      "INFO:tensorflow:loss = 1.7694, step = 1001 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.871\n",
      "INFO:tensorflow:loss = 0.0329611, step = 1101 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 567.266\n",
      "INFO:tensorflow:loss = 0.300987, step = 1201 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.516\n",
      "INFO:tensorflow:loss = 4.51052, step = 1301 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 556.7\n",
      "INFO:tensorflow:loss = 0.452854, step = 1401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.216\n",
      "INFO:tensorflow:loss = 3.65891, step = 1501 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.371\n",
      "INFO:tensorflow:loss = 0.0616768, step = 1601 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 575.017\n",
      "INFO:tensorflow:loss = 2.8113, step = 1701 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.934\n",
      "INFO:tensorflow:loss = 3.71684, step = 1801 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 556.709\n",
      "INFO:tensorflow:loss = 0.102785, step = 1901 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.772\n",
      "INFO:tensorflow:loss = 8.84164, step = 2001 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 539.471\n",
      "INFO:tensorflow:loss = 0.572075, step = 2101 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 547.193\n",
      "INFO:tensorflow:loss = 9.59558, step = 2201 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.768\n",
      "INFO:tensorflow:loss = 1.11815, step = 2301 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.362\n",
      "INFO:tensorflow:loss = 15.8662, step = 2401 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.47\n",
      "INFO:tensorflow:loss = 1.52681, step = 2501 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 548.83\n",
      "INFO:tensorflow:loss = 5.46087, step = 2601 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.926\n",
      "INFO:tensorflow:loss = 2.21388, step = 2701 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.603\n",
      "INFO:tensorflow:loss = 8.71692, step = 2801 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.599\n",
      "INFO:tensorflow:loss = 0.063797, step = 2901 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.072\n",
      "INFO:tensorflow:loss = 2.63302, step = 3001 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.572\n",
      "INFO:tensorflow:loss = 6.09035, step = 3101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.134\n",
      "INFO:tensorflow:loss = 8.06524, step = 3201 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.991\n",
      "INFO:tensorflow:loss = 1.55295, step = 3301 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.9\n",
      "INFO:tensorflow:loss = 0.611963, step = 3401 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.962\n",
      "INFO:tensorflow:loss = 2.34356, step = 3501 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.853\n",
      "INFO:tensorflow:loss = 0.457089, step = 3601 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 567.28\n",
      "INFO:tensorflow:loss = 0.234941, step = 3701 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.531\n",
      "INFO:tensorflow:loss = 6.9943, step = 3801 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.239\n",
      "INFO:tensorflow:loss = 4.71715, step = 3901 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.288\n",
      "INFO:tensorflow:loss = 6.01809, step = 4001 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.899\n",
      "INFO:tensorflow:loss = 0.154114, step = 4101 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 551.545\n",
      "INFO:tensorflow:loss = 5.84314, step = 4201 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.422\n",
      "INFO:tensorflow:loss = 0.0336857, step = 4301 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.119\n",
      "INFO:tensorflow:loss = 99.249, step = 4401 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 505.502\n",
      "INFO:tensorflow:loss = 4.43755, step = 4501 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.183\n",
      "INFO:tensorflow:loss = 1.23779, step = 4601 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 465.116\n",
      "INFO:tensorflow:loss = 2.71479, step = 4701 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 586.098\n",
      "INFO:tensorflow:loss = 5.76619, step = 4801 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 568.359\n",
      "INFO:tensorflow:loss = 5.25433, step = 4901 (0.176 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into abalone_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.13746.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-02-18:42:54\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-02-18:42:56\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 5.86984, rmse = 2.42278\n",
      "INFO:tensorflow:Restoring parameters from abalone_output/model.ckpt-5000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: abalone_output/export/Servo/1501699376/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine local train --package-path trainer \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code on CMLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [abalone_170802_184256] submitted successfully.\n",
      "INFO\t2017-08-02 18:42:58 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2017-08-02 18:42:58 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2017-08-02 18:42:58 +0000\tservice\t\tJob abalone_170802_184256 is queued.\n",
      "INFO\t2017-08-02 18:42:58 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2017-08-02 18:42:59 +0000\tservice\t\tWaiting for TensorFlow to start.\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"ps\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"ps\", \"index\": 2} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"master\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"worker\", \"index\": 0} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"worker\", \"index\": 1} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"worker\", \"index\": 2} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"ps\", \"index\": 1} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:00 +0000\tps-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\tRunning task with arguments: --cluster={\"master\": [\"master-288261a57a-0:2222\"], \"ps\": [\"ps-288261a57a-0:2222\", \"ps-288261a57a-1:2222\", \"ps-288261a57a-2:2222\"], \"worker\": [\"worker-288261a57a-0:2222\", \"worker-288261a57a-1:2222\", \"worker-288261a57a-2:2222\", \"worker-288261a57a-3:2222\"]} --task={\"type\": \"worker\", \"index\": 3} --job={\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"scale_tier\": \"STANDARD_1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"package_uris\": [\"gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"python_module\": \"trainer.task\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"args\": [\"--train-files\", \"gs://smiling-beaming-abalone/abalone_train.csv\", \"--eval-files\", \"gs://smiling-beaming-abalone/abalone_test.csv\", \"--train-steps\", \"5000\", \"--eval-steps\", \"100\"],\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"region\": \"us-central1\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"runtime_version\": \"1.2\",\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t  \"job_dir\": \"gs://smiling-beaming-abalone/abalone_170802_184256\"\n",
      "INFO\t2017-08-02 18:43:00 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:00 +0000\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-0\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-0\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-0\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-2\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-2\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-2\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tps-replica-1\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:01 +0000\tps-replica-1\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tps-replica-1\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-3\t\tRunning module trainer.task.\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-3\t\tDownloading the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tworker-replica-3\t\tRunning command: gsutil -q cp gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tps-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tps-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tmaster-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:01 +0000\tmaster-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-0\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-0\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-2\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-2\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-1\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-1\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:02 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-3\t\tInstalling the package: gs://smiling-beaming-abalone/abalone_170802_184256/packages/a6f5e9c2007256c1907b1b81d94436c2baf356b004627558e22b4c9b732429d7/trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-3\t\tRunning command: pip install --user --upgrade --force-reinstall --no-deps trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tcreating '/tmp/tmphLULrwpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:02 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:02 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tps-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:02 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpEiRxKspip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tcreating '/tmp/tmpEo3bMxpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tcreating '/tmp/tmpBQb4SOpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tcreating '/tmp/tmpjFnykYpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tmaster-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tcreating '/tmp/tmppAsii5pip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tcreating '/tmp/tmpGMM7jvpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-2\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tInstalling collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-3\t\tSuccessfully installed trainer-0.0.0\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-1\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tworker-replica-0\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tcreating '/tmp/tmp4C8_efpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:03 +0000\tps-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-3\t\tRunning command: pip install --user trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-2\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-1\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-3\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-0\t\tProcessing ./trainer-0.0.0.tar.gz\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-2\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-2\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tcreating '/tmp/tmpPGABCRpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tcreating '/tmp/tmpMKuAKjpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tmaster-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tcreating '/tmp/tmpbELhFOpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:04 +0000\tps-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-1\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-1\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-3\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-3\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:04 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tcreating '/tmp/tmpuSRUXRpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\t  Requirement already satisfied (use --upgrade to upgrade): trainer==0.0.0 from file:///user_dir/trainer-0.0.0.tar.gz in /root/.local/lib/python2.7/site-packages\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tBuilding wheels for collected packages: trainer\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: started\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-2\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tcreating '/tmp/tmpMkrOvvpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tcreating '/tmp/tmpWowo9lpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tSuccessfully built trainer\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-3\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-1\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tcreating '/tmp/tmpFk4x5zpip-wheel-/trainer-0.0.0-cp27-none-any.whl' and adding '.' to it\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer/task.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer/__init__.py'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/DESCRIPTION.rst'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/metadata.json'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/top_level.txt'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/WHEEL'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/METADATA'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tadding 'trainer-0.0.0.dist-info/RECORD'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\t  Running setup.py bdist_wheel for trainer: finished with status 'done'\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/0d/1b/db/f8e86b296734f0b137e17e5d34862f4ae4faf8388755c6272f\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tSuccessfully built trainer\n",
      "WARNING\t2017-08-02 18:43:05 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f56f57c8290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:05 +0000\tps-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-288261a57a-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:05 +0000\tps-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:05 +0000\tworker-replica-0\t\tRunning command: python -m trainer.task --train-files gs://smiling-beaming-abalone/abalone_train.csv --eval-files gs://smiling-beaming-abalone/abalone_test.csv --train-steps 5000 --eval-steps 100 --job-dir gs://smiling-beaming-abalone/abalone_170802_184256\n",
      "WARNING\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0362f08290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://master-288261a57a-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:06 +0000\tmaster-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tmaster-replica-0\t\tFrom /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tmaster-replica-0\t\tMonitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6f3448290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-1\t\t}\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-288261a57a-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tps-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f324a704290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\t}\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-288261a57a-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-2\t\tWaiting 9 secs before starting training.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tmaster-replica-0\t\tCreate CheckpointSaverHook.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-2\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'ps', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f534bcc8290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-2\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-2\t\t}\n",
      "INFO\t2017-08-02 18:43:07 +0000\tps-replica-2\t\t, '_num_worker_replicas': 4, '_task_id': 2, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://ps-288261a57a-2:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tps-replica-2\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7feaa5648290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\t}\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\t, '_num_worker_replicas': 4, '_task_id': 3, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-288261a57a-3:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-3\t\tWaiting 14 secs before starting training.\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f28c0d88290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\t}\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\t, '_num_worker_replicas': 4, '_task_id': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-288261a57a-1:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:07 +0000\tworker-replica-1\t\tWaiting 4 secs before starting training.\n",
      "WARNING\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\tUsing config: {'_model_dir': 'gs://smiling-beaming-abalone/abalone_170802_184256', '_save_checkpoints_secs': 600, '_num_ps_replicas': 3, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6687648290>, '_tf_config': gpu_options {\n",
      "INFO\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\t  per_process_gpu_memory_fraction: 1.0\n",
      "INFO\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\t}\n",
      "INFO\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\t, '_num_worker_replicas': 4, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://worker-288261a57a-0:2222', '_session_config': None}\n",
      "WARNING\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\tuid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO\t2017-08-02 18:43:08 +0000\tworker-replica-0\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 18:43:09 +0000\tworker-replica-0\t\tloss = 167.933, step = 0\n",
      "INFO\t2017-08-02 18:43:09 +0000\tworker-replica-0\t\tloss = 8.79835, step = 100 (0.522 sec)\n",
      "INFO\t2017-08-02 18:43:10 +0000\tworker-replica-0\t\tloss = 6.42701, step = 200 (0.522 sec)\n",
      "INFO\t2017-08-02 18:43:10 +0000\tmaster-replica-0\t\tSaving checkpoints for 205 into gs://smiling-beaming-abalone/abalone_170802_184256/model.ckpt.\n",
      "INFO\t2017-08-02 18:43:10 +0000\tworker-replica-0\t\tloss = 4.54123, step = 301 (0.523 sec)\n",
      "INFO\t2017-08-02 18:43:11 +0000\tworker-replica-0\t\tloss = 2.59619, step = 401 (0.545 sec)\n",
      "INFO\t2017-08-02 18:43:11 +0000\tworker-replica-0\t\tloss = 7.66773, step = 501 (0.535 sec)\n",
      "INFO\t2017-08-02 18:43:12 +0000\tworker-replica-0\t\tloss = 0.905795, step = 601 (0.458 sec)\n",
      "INFO\t2017-08-02 18:43:12 +0000\tworker-replica-0\t\tloss = 7.31026, step = 701 (0.371 sec)\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-1\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-0\t\tloss = 0.0602242, step = 801 (0.381 sec)\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-1\t\tloss = 18.2042, step = 841\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-0\t\tloss = 3.88204, step = 948 (0.384 sec)\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-1\t\tloss = 1.32165, step = 1054 (0.356 sec)\n",
      "INFO\t2017-08-02 18:43:13 +0000\tworker-replica-0\t\tloss = 10.3817, step = 1157 (0.381 sec)\n",
      "INFO\t2017-08-02 18:43:14 +0000\tworker-replica-1\t\tloss = 9.74785, step = 1241 (0.357 sec)\n",
      "INFO\t2017-08-02 18:43:14 +0000\tworker-replica-0\t\tloss = 0.222659, step = 1363 (0.394 sec)\n",
      "INFO\t2017-08-02 18:43:14 +0000\tworker-replica-1\t\tloss = 1.56143, step = 1438 (0.371 sec)\n",
      "INFO\t2017-08-02 18:43:14 +0000\tworker-replica-0\t\tloss = 0.365621, step = 1554 (0.367 sec)\n",
      "INFO\t2017-08-02 18:43:14 +0000\tworker-replica-1\t\tloss = 5.29593, step = 1650 (0.412 sec)\n",
      "INFO\t2017-08-02 18:43:15 +0000\tworker-replica-0\t\tloss = 0.609581, step = 1753 (0.393 sec)\n",
      "INFO\t2017-08-02 18:43:15 +0000\tworker-replica-1\t\tloss = 4.80658, step = 1843 (0.391 sec)\n",
      "INFO\t2017-08-02 18:43:15 +0000\tworker-replica-0\t\tloss = 1.13836, step = 1962 (0.412 sec)\n",
      "INFO\t2017-08-02 18:43:15 +0000\tworker-replica-1\t\tloss = 2.23894, step = 2040 (0.378 sec)\n",
      "INFO\t2017-08-02 18:43:15 +0000\tworker-replica-0\t\tloss = 62.3914, step = 2157 (0.375 sec)\n",
      "INFO\t2017-08-02 18:43:16 +0000\tworker-replica-1\t\tloss = 13.3873, step = 2251 (0.410 sec)\n",
      "INFO\t2017-08-02 18:43:16 +0000\tworker-replica-0\t\tloss = 14.9771, step = 2350 (0.372 sec)\n",
      "INFO\t2017-08-02 18:43:16 +0000\tworker-replica-1\t\tloss = 1.59937, step = 2453 (0.382 sec)\n",
      "INFO\t2017-08-02 18:43:16 +0000\tworker-replica-0\t\tloss = 0.842644, step = 2550 (0.380 sec)\n",
      "INFO\t2017-08-02 18:43:16 +0000\tworker-replica-1\t\tloss = 1.27023, step = 2651 (0.391 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-0\t\tloss = 3.12425, step = 2749 (0.398 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-1\t\tloss = 3.46066, step = 2856 (0.403 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-0\t\tloss = 0.133182, step = 2942 (0.376 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-2\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-1\t\tloss = 0.000418032, step = 3065 (0.388 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-0\t\tloss = 13.2346, step = 3134 (0.346 sec)\n",
      "INFO\t2017-08-02 18:43:17 +0000\tworker-replica-2\t\tloss = 1.27738, step = 3136\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-1\t\tloss = 8.51821, step = 3334 (0.392 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-0\t\tloss = 20.8663, step = 3407 (0.359 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-2\t\tloss = 0.558218, step = 3449 (0.362 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-1\t\tloss = 0.0486085, step = 3656 (0.401 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-2\t\tloss = 2.47265, step = 3745 (0.373 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-1\t\tloss = 7.46858, step = 3964 (0.395 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-0\t\tloss = 0.445814, step = 4005 (0.383 sec)\n",
      "INFO\t2017-08-02 18:43:18 +0000\tworker-replica-2\t\tloss = 5.56299, step = 4028 (0.361 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-1\t\tloss = 6.54367, step = 4269 (0.399 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-0\t\tloss = 3.96589, step = 4308 (0.400 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-2\t\tloss = 0.0214022, step = 4323 (0.391 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-1\t\tloss = 14.1335, step = 4576 (0.404 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-0\t\tloss = 0.159633, step = 4596 (0.375 sec)\n",
      "INFO\t2017-08-02 18:43:19 +0000\tworker-replica-2\t\tloss = 7.90404, step = 4627 (0.396 sec)\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-1\t\tloss = 6.0745, step = 4886 (0.394 sec)\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-0\t\tloss = 1.92899, step = 4892 (0.376 sec)\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-2\t\tloss = 0.732315, step = 4923 (0.373 sec)\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-2\t\tLoss for final step: 1.20904.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-1\t\tLoss for final step: 0.703241.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-0\t\tLoss for final step: 1.71097.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-2\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-0\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:43:20 +0000\tworker-replica-1\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 18:43:23 +0000\tworker-replica-3\t\tCreate CheckpointSaverHook.\n",
      "INFO\t2017-08-02 18:43:27 +0000\tmaster-replica-0\t\tloss = 0.722282, step = 205\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-02-18:43:28\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_184256/model.ckpt-205\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-02 18:43:28 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-02-18:43:29\n",
      "INFO\t2017-08-02 18:43:29 +0000\tmaster-replica-0\t\tSaving dict for global step 359: global_step = 359, loss = 7.86183, rmse = 2.8039\n",
      "INFO\t2017-08-02 18:43:32 +0000\tmaster-replica-0\t\tValidation (step 203): loss = 7.86183, global_step = 359, rmse = 2.8039\n",
      "INFO\t2017-08-02 18:43:32 +0000\tmaster-replica-0\t\tglobal_step/sec: 1017.47\n",
      "INFO\t2017-08-02 18:43:32 +0000\tmaster-replica-0\t\tSaving checkpoints for 5005 into gs://smiling-beaming-abalone/abalone_170802_184256/model.ckpt.\n",
      "INFO\t2017-08-02 18:43:42 +0000\tmaster-replica-0\t\tLoss for final step: 0.103867.\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tStarting evaluation at 2017-08-02-18:43:43\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_184256/model.ckpt-5005\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [1/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [2/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [3/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [4/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [5/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [6/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [7/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [8/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [9/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [10/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [11/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [12/100]\n",
      "INFO\t2017-08-02 18:43:43 +0000\tmaster-replica-0\t\tEvaluation [13/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [14/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [15/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [16/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [17/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [18/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [19/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [20/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [21/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [22/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [23/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [24/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [25/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [26/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [27/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [28/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [29/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [30/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [31/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [32/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [33/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [34/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [35/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [36/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [37/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [38/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [39/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [40/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [41/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [42/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [43/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [44/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [45/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [46/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [47/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [48/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [49/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [50/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [51/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [52/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [53/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [54/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [55/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [56/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [57/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [58/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [59/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [60/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [61/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [62/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [63/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [64/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [65/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [66/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [67/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [68/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [69/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [70/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [71/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [72/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [73/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [74/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [75/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [76/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [77/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [78/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [79/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [80/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [81/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [82/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [83/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [84/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [85/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [86/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [87/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [88/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [89/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [90/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [91/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [92/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [93/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [94/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [95/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [96/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [97/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [98/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [99/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tEvaluation [100/100]\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tFinished evaluation at 2017-08-02-18:43:44\n",
      "INFO\t2017-08-02 18:43:44 +0000\tmaster-replica-0\t\tSaving dict for global step 5005: global_step = 5005, loss = 6.03985, rmse = 2.45761\n",
      "INFO\t2017-08-02 18:43:46 +0000\tmaster-replica-0\t\tRestoring parameters from gs://smiling-beaming-abalone/abalone_170802_184256/model.ckpt-5005\n",
      "INFO\t2017-08-02 18:43:48 +0000\tmaster-replica-0\t\tAssets added to graph.\n",
      "INFO\t2017-08-02 18:43:48 +0000\tmaster-replica-0\t\tNo assets to write.\n",
      "INFO\t2017-08-02 18:43:53 +0000\tmaster-replica-0\t\tSavedModel written to: gs://smiling-beaming-abalone/abalone_170802_184256/export/Servo/1501699426/saved_model.pb\n",
      "INFO\t2017-08-02 18:43:54 +0000\tmaster-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:43:54 +0000\tmaster-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:43:54 +0000\tmaster-replica-0\t\tTask completed successfully.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tservice\t\tTearing down TensorFlow.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-0\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-0\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-0\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-1\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-1\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-1\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-2\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-2\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tps-replica-2\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tworker-replica-3\t\tSignal 15 (SIGTERM) was caught. Terminated by service. This is normal behavior.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tworker-replica-3\t\tModule completed; cleaning up.\n",
      "INFO\t2017-08-02 18:44:00 +0000\tworker-replica-3\t\tClean up finished.\n",
      "INFO\t2017-08-02 18:45:30 +0000\tservice\t\tFinished tearing down TensorFlow.\n",
      "INFO\t2017-08-02 18:45:30 +0000\tservice\t\tJob completed successfully.\n",
      "endTime: '2017-08-02T18:44:00'\n",
      "jobId: abalone_170802_184256\n",
      "startTime: '2017-08-02T18:43:30'\n",
      "state: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.2 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path trainer \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
