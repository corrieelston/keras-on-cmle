{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Neural Net (Local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our code, locally to begin with, using CMLE.\n",
    "\n",
    "Here are the changes we need to make convert our code to a python script file in folder called trainer and convert that folder into a package by adding an __init__.py script file.\n",
    "\n",
    "After that we'll test the code from the command line.\n",
    "\n",
    "Then we'll run the full, distributed job on CMLE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some housekeeping, we need to delete the output folder we use locally between runs (in case you run this notebook multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf abalone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p trainer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!touch trainer2/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer2/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer2/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import six\n",
    "from six.moves import urllib\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CSV_COLUMNS = ['length', 'diameter', 'height', 'whole_weight', 'shucked_weight',\n",
    "               'viscera_weight', 'shell_weight', 'num_rings']\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0]]\n",
    "\n",
    "PREDICTED_COLUMN = 'num_rings'\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('length'),\n",
    "    tf.feature_column.numeric_column('diameter'),\n",
    "    tf.feature_column.numeric_column('height'),\n",
    "    tf.feature_column.numeric_column('whole_weight'),\n",
    "    tf.feature_column.numeric_column('shucked_weight'),\n",
    "    tf.feature_column.numeric_column('viscera_weight'),\n",
    "    tf.feature_column.numeric_column('shell_weight'),\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - {PREDICTED_COLUMN}\n",
    "\n",
    "def parse_csv(rows_string_tensor):\n",
    "  columns = tf.decode_csv(rows_string_tensor, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "  features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "  for col in UNUSED_COLUMNS:\n",
    "    features.pop(col)\n",
    "\n",
    "  for key, value in six.iteritems(features):\n",
    "    features[key] = tf.expand_dims(features[key], -1)\n",
    "  return features\n",
    "\n",
    "def generate_input_fn(filenames,\n",
    "                      num_epochs=None,\n",
    "                      shuffle=True,\n",
    "                      skip_header_lines=0,\n",
    "                      batch_size=64):\n",
    "  \n",
    "  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=shuffle)\n",
    "  reader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n",
    "\n",
    "  _, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "\n",
    "  features = parse_csv(rows)\n",
    "\n",
    "  if shuffle:\n",
    "    features = tf.train.shuffle_batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        min_after_dequeue=2 * batch_size + 1,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "  else:\n",
    "    features = tf.train.batch(\n",
    "        features,\n",
    "        batch_size,\n",
    "        capacity=batch_size * 10,\n",
    "        num_threads=multiprocessing.cpu_count(),\n",
    "        enqueue_many=True,\n",
    "        allow_smaller_final_batch=True\n",
    "    )\n",
    "\n",
    "  return features, features.pop(PREDICTED_COLUMN)\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense\n",
    "\n",
    "def generate_model_fn(learning_rate):\n",
    "    \n",
    "    def _model_fn(mode, features, labels):\n",
    "\n",
    "        (length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight) = INPUT_COLUMNS\n",
    "\n",
    "        transformed_columns = [\n",
    "            length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight\n",
    "        ]\n",
    "\n",
    "        inputs = tf.feature_column.input_layer(features, transformed_columns)\n",
    "\n",
    "        first_hidden_layer = Dense(10, activation='relu')(inputs)\n",
    "        second_hidden_layer = Dense(10, activation='relu')(first_hidden_layer)\n",
    "        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n",
    "\n",
    "        if mode in (Modes.PREDICT, Modes.EVAL):\n",
    "          predictions = tf.reshape(output_layer, [-1])\n",
    "          predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "        if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "          loss = tf.losses.mean_squared_error(labels, output_layer)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "          train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "        \n",
    "        if mode == Modes.TRAIN:\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        if mode == Modes.EVAL:\n",
    "          eval_metric_ops = {\n",
    "              \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "                  tf.cast(labels, tf.float32), predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "        \n",
    "        if mode == Modes.PREDICT:\n",
    "          export_outputs = {\n",
    "              'prediction': tf.estimator.export.RegressionOutput(predictions)\n",
    "          }\n",
    "          return tf.estimator.EstimatorSpec(\n",
    "              mode, predictions=predictions_dict, export_outputs=export_outputs)\n",
    "    \n",
    "    return _model_fn\n",
    "  \n",
    "def generate_experiment_fn(**experiment_args):  \n",
    "  \n",
    "  def _experiment_fn(run_config, hparams):\n",
    "\n",
    "    train_input = lambda: generate_input_fn(\n",
    "      hparams.train_files,\n",
    "      num_epochs=hparams.num_epochs,\n",
    "      batch_size=hparams.train_batch_size,\n",
    "    )\n",
    "\n",
    "    test_input = lambda: generate_input_fn(\n",
    "      hparams.eval_files,\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        tf.estimator.Estimator(\n",
    "            generate_model_fn(learning_rate=hparams.learning_rate),\n",
    "            config=run_config\n",
    "        ),\n",
    "        train_input_fn=train_input,\n",
    "        eval_input_fn=test_input,\n",
    "        **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn\n",
    "\n",
    "def example_serving_input_fn():\n",
    "  \"\"\"Build the serving inputs.\"\"\"\n",
    "  example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "  )\n",
    "  features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n",
    "  )\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS or local paths to training data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        required=True\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate for the optimizer',\n",
    "        default=0.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=[\n",
    "            'DEBUG',\n",
    "            'ERROR',\n",
    "            'FATAL',\n",
    "            'INFO',\n",
    "            'WARN'\n",
    "        ],\n",
    "        default='INFO',\n",
    "        help='Set logging verbosity'\n",
    "    )\n",
    "    # Experiment arguments\n",
    "    parser.add_argument(\n",
    "        '--eval-delay-secs',\n",
    "        help='How long to wait before running first evaluation',\n",
    "        default=10,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min-eval-frequency',\n",
    "        help='Minimum number of training steps between evaluations',\n",
    "        default=1,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help=\"\"\"\\\n",
    "        Number of steps to run evalution for at each checkpoint.\n",
    "        If unspecified will run until the input from --eval-files is exhausted\n",
    "        \"\"\",\n",
    "        default=None,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args([\n",
    "        '--train-files', 'gs://smiling-beaming-abalone/abalone_train.csv',\n",
    "        '--eval-files', 'gs://smiling-beaming-abalone/abalone_test.csv',\n",
    "        '--job-dir', 'abalone_output',\n",
    "        '--train-steps', '5000',\n",
    "        '--eval-steps', '100'\n",
    "      ])\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # Set python level verbosity\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    # Set C++ Graph Execution level verbosity\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "        tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "    # Run the training job\n",
    "    # learn_runner pulls configuration information from environment\n",
    "    # variables using tf.learn.RunConfig and uses this configuration\n",
    "    # to conditionally execute Experiment, or param server code\n",
    "    learn_runner.run(\n",
    "        generate_experiment_fn(\n",
    "            min_eval_frequency=args.min_eval_frequency,\n",
    "            eval_delay_secs=args.eval_delay_secs,\n",
    "            train_steps=args.train_steps,\n",
    "            eval_steps=args.eval_steps,\n",
    "            export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "                example_serving_input_fn,\n",
    "                exports_to_keep=1\n",
    "            )]\n",
    "        ),\n",
    "        run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n",
    "        hparams=hparam.HParams(**args.__dict__)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the code locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gcloud ml-engine local train --package-path trainer2 \\\n",
    "   --module-name trainer.task \\\n",
    "   -- \\\n",
    "   --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "   --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "   --job-dir abalone_output \\\n",
    "   --train-steps 5000 \\\n",
    "   --eval-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code on CMLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [abalone_170802_181949] submitted successfully.\n",
      "INFO\t2017-08-02 18:19:51 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2017-08-02 18:19:51 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2017-08-02 18:19:51 +0000\tservice\t\tJob abalone_170802_181949 is queued.\n",
      "INFO\t2017-08-02 18:20:00 +0000\tservice\t\tWaiting for job to be provisioned.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs submit training abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --stream-logs \\\n",
    "  --scale-tier STANDARD_1 \\\n",
    "  --runtime-version 1.0 \\\n",
    "  --job-dir gs://smiling-beaming-abalone/abalone_$(date -u +%y%m%d_%H%M%S) \\\n",
    "  --module-name trainer2.task \\\n",
    "  --package-path trainer2 \\\n",
    "  --region us-central1 \\\n",
    "  -- \\\n",
    "  --train-files gs://smiling-beaming-abalone/abalone_train.csv \\\n",
    "  --eval-files gs://smiling-beaming-abalone/abalone_test.csv \\\n",
    "  --train-steps 5000 \\\n",
    "  --eval-steps 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
